{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cross-entropy compares two probability distributions.",
   "id": "c07843693f731461"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T05:07:50.654043Z",
     "start_time": "2025-03-17T05:07:50.489953Z"
    }
   },
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
    "         math.log(softmax_output[1]) * target_output[1] +\n",
    "         math.log(softmax_output[2]) * target_output[2])\n",
    "\n",
    "loss"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That is the full categorical cross-entropy calculation, but we can make a few assumptions given one-hot target vectors. First, the values for `target_output[1]` and `target_ouput[2]` in this case are both `0`, and anything multiplied by `0` is `0`. Thus, we don't need to calute these indices. Next, the value for `target_output[0]` in this case is 1. So this can be omitted as any number multiplied by `1` remains the same.",
   "id": "f159d261ed1810a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T04:53:54.088659Z",
     "start_time": "2025-03-17T04:53:54.077680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = -math.log(softmax_output[0])\n",
    "loss"
   ],
   "id": "784f394d044c268f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Consider a scenario with a neural network that performs classification between three classes, and the neural network classifies in batches of three. After running through the softmax activation function with a batch of 3 samples and 3 classes, the network's output layer yields:",
   "id": "ec49caa03b0a8a83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:10:29.381929Z",
     "start_time": "2025-03-17T05:10:29.372239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                    [0.1, 0.5, 0.4],\n",
    "                    [0.02, 0.9, 0.08]]\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "# we can map these target indices to retrieve the va;ues from the softmax distribution\n",
    "for targ_inx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_inx])"
   ],
   "id": "a00ecbb750d02821",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `zip()` function lets us iterate over multiple iterables at the same time in Python. This can be further simplified using NumPy (we're creating a NumPy array of the Softmax outputs this time):",
   "id": "74bd70e6df5d8b3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:13:47.658931Z",
     "start_time": "2025-03-17T05:13:47.646739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "softmax_outputs[[0, 1, 2], class_targets]"
   ],
   "id": "5cb00d7a95a30d92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.5, 0.9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The list `[0, 1, 2]` is used to filter the first dimension. This dimension contains the predictions and we want to retain them all. We can achieve that by using a list containing numbers from `0` through all of the indices. We know we're going to have as many indices as distributions in our entire batch, so we can use a `range()` instead of hard-coding:",
   "id": "940f2c0f06dadf3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:18:54.493897Z",
     "start_time": "2025-03-17T05:18:54.482390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "f"
   ],
   "id": "2e9d335a7e5dc5e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.5, 0.9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:21:07.739982Z",
     "start_time": "2025-03-17T05:21:07.728857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply negative log\n",
    "neg_log = -np.log(f)\n",
    "neg_log"
   ],
   "id": "f38267b9ade8bfe3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35667494, 0.69314718, 0.10536052])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we want an average loss per batch to have an idea about how our model is doing during training.",
   "id": "6f5f3362cf011662"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:21:28.504911Z",
     "start_time": "2025-03-17T05:21:28.495021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "average_loss = np.mean(neg_log)\n",
    "average_loss"
   ],
   "id": "452c6393135aecda",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.38506088005216804)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have learned that targets can be one-hot encoded, where all values, except for one, are zeros, and the correct label's position is filled with 1. That can also be sparse, which means that the numbers they contain are the correct class numbers - we are generating them this way with the `spiral_data()` function, and we can allow the loss calculation to accept any of these forms. Since we implemented this to work with sparse labels (as in our training data), we have to add a check if they are one-hot encoded and handle it a bit differently in this new case. The check can be performed by counting the dimensions - if targets are single-dimensional (like a list), they are sparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors. In this second case, instead of filtering out the confidences at the target labels, we have to multiply confidences by the targets, zeroing out all values except the ones at correct labels, performing a sum along the row axis (axis 1).",
   "id": "f125215ad96d9529"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:36:25.294534Z",
     "start_time": "2025-03-17T05:36:25.280478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                            [0, 1, 0],\n",
    "                            [0, 1, 0]])\n",
    "\n",
    "# Probabilities for target values - only if categorical labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "# Mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs * class_targets,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "print(neg_log)\n",
    "\n",
    "avg_loss = np.mean(neg_log)\n",
    "avg_loss"
   ],
   "id": "8623b078d2462074",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.38506088005216804)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "From a mathematical point of view, `log(0)` is undefined. We already know the following dependence: if `y=log(0)`, then `e^y=x`. The question of what the resulting y is in `y=log(0)` is the same as the question of what's the `y` in `e^y=0`. In simplified term, the constant `e` to any power is always a positive number, and there is no `y` resulting in `e^y=0`. This means the `log(0)` is undefined. We need to be aware of what the `log(0)` is, and \"undefined\" does not mean that we don't know anything about it. Since `log(0)` is undefined, what's the result for a value very close to `0`?\n",
    "It's \" the limit of a natural logarithm of `x`, with `x` approaching `0` from a positive equals negative infinity. What this means is that the limit is negative infinity for an infintely small `x`, where `x` never reaches `0`.\n",
    "The situation is a bit different in programming languages. We do not have limites hre, just a function which, given a parameter, returns some value. The negative natural logarith of `0`, in Python wiht NumPy, equals an infinitely big number, rather than undefined, and prints a wanting about division by `0`. If `-np.log(0)` equals `inf`, is it possible to calculate e to the power of negative infinity?"
   ],
   "id": "649466d576ed40d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:06:14.835327Z",
     "start_time": "2025-03-18T05:06:14.823937Z"
    }
   },
   "cell_type": "code",
   "source": "np.e**(-np.inf)\n",
   "id": "a7d0584b02bcfaee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T04:57:15.734848Z",
     "start_time": "2025-03-18T04:57:15.721949Z"
    }
   },
   "cell_type": "code",
   "source": "-np.log(0).item()",
   "id": "38d562d32d50eaec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linho\\AppData\\Local\\Temp\\ipykernel_8836\\1019113829.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  -np.log(0).item()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:07:59.257139Z",
     "start_time": "2025-03-18T05:07:59.244990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A single infinite value in a list will cause the average of that list to also be infinite:\n",
    "np.mean([1, 2, 3, -np.log(0)])"
   ],
   "id": "715664eb1f9d016e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linho\\AppData\\Local\\Temp\\ipykernel_8836\\1152205266.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  np.mean([1, 2, 3, -np.log(0)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(inf)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:08:39.407164Z",
     "start_time": "2025-03-18T05:08:39.395736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We could ad a very small value to the confidence to prevent it from being a zero:\n",
    "-np.log(1e-7).item()"
   ],
   "id": "d02d2e53a65a699e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.11809565095832"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Adding a very small value, one-tenth of a million, to the confidence at its far edge will insignificantly impact the result, but this method yields an additional 2 issues. First, in the case where confidence value is `1`:",
   "id": "38494a93c54f364f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:11:15.341404Z",
     "start_time": "2025-03-18T05:11:15.331076Z"
    }
   },
   "cell_type": "code",
   "source": "-np.log(1 + 1e-7).item()",
   "id": "1659fa910b328504",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.999999505838704e-08"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When the model is fully correct in a prediction and puts all the confidence in the correct label, loss becomes a negative value instead of being `0`. the other probelm here is shifting confidence towards `1`, even if by a very small value. To prevent both issues, it's better to clip values from both sides by the same number, `1e-7` in our case. That means that the lowest possible will become `1e-7` but the highest possible value, instead of being `1 + 1e-7`, will become `1 - 1e-7`",
   "id": "3743aae1c1cbe827"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:14:05.742117Z",
     "start_time": "2025-03-18T05:14:05.730988Z"
    }
   },
   "cell_type": "code",
   "source": "-np.log(1 - 1e-7).item()",
   "id": "ac5da83f3457934e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000494736474e-07"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Common Loss Class",
   "id": "c4b119c02c11dd57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:40:08.328284Z",
     "start_time": "2025-03-18T05:40:08.299525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss:\n",
    "\n",
    "    # Calculate the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss"
   ],
   "id": "5536ef2b79720bbb",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross-entropy loss",
   "id": "f821d2f573278784"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:47:06.104187Z",
     "start_time": "2025-03-18T05:47:06.098533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This class inherits the `Loss` class and performs all the error calculations that we derived throughout this notebook and can be used as an object.\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "\n",
    "        return negative_log_likelihoods"
   ],
   "id": "7e2d74f935f8b4ab",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T05:47:59.234482Z",
     "start_time": "2025-03-18T05:47:59.229310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "loss.item()"
   ],
   "id": "80f6abaf2e81caec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Accuracy Calculation\n",
    "\n",
    "While loss is a useful metric for optimizing a model, the metric commonly used in practice along with loss is the accuracy, which describes how often the largest confidence is the correct class in terms of a fraction. Conveniently, we can reuse existing variable definitions to calculate the accuracy metric. We will use the `argmax` values from the `softmax outputs` and then compare these to the targets. We have to modify the `softmax_outputs` slightly for this purpose:"
   ],
   "id": "1b3c65b7eff2b210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T06:06:55.930871Z",
     "start_time": "2025-03-18T06:06:55.924625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions==class_targets)\n",
    "\n",
    "accuracy\n"
   ],
   "id": "b5087b624e9b9c9f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6666666666666666)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d09da44616d79217"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
