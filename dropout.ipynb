{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Another option for neural network regularization is adding a **dropout layer**. This type of layer disables some neurons, while the others pass through unchanged. The idea here, similarly to regularization, is to prevent a neural network from becoming too dependent on any neuron or for any neuron to be relied upon entirely in a specific instance (which can be common if a model overfits the training data). Another problem dropout can help with is **co-adoption**, which happens when neurons depend on the output values of other neurons and do not learn the underlying function on their onw. Dropout can also help with **noise** and other pertubations in the training data as more neurons working together mean that the model can learn more complex functions.\n",
    "\n",
    "The Dropout function works by randomly disabling neurons at a given rate during every forward pass, forcing the network to learn how to make accurate predictions with only a random part of neurons remaining. Dropout forces the model to use more neurons for the same purpose, resulting in a higher chance of learning the underlying function that describes the data. For example, if we disbale one half of the neurons during the current step, and the other half during the next step, we are forcing more neurons to learn the data, as only a part of them \"see\" the data and gets updates in a given pass. These alternating halves of neurons are an example, and in reality, we'll use a hyperparameter to inform the dropout layer of the number of neurons to disable randomly.\n",
    "\n",
    "Also, since active neurons are changing, dropout helps prevent overfitting, as the model can't use specific neurons to memorize certain samples. It's also worth mentioning that the dropuout layer does not truly disable neurons, but instead zeros their output. In other words, dropout does not decrease the number of neurons used, nor does it make the training process twice as fast when half the neurons are disabled."
   ],
   "id": "e97313bacdce4ae8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Forward Pass",
   "id": "c00b0e824d339405"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the code, we will \"turn off\" neurons with a filter that is an array with the same shape as the layer output but filled with numbers drawn from a Nernoulli distribution. A **Bernoulli distribution** is a binary (or discrete) probability distribution where we can get a value of *1* with a probability of *p* and value of *0* with a probability of *q*.\n",
    "\n",
    "$$\n",
    "P(r_i = 1) = p $$\n",
    "$$\n",
    "P(r_i = 0) = q = 1 - p = 1- P(r_i) = 1\n",
    "$$\n",
    "\n",
    "What this means is that the probability of this value being *1* is *p*. The probability of it being *0* is *q = 1 -p*. Therefore:\n",
    "\n",
    "$$ r_i ~ Bernoulli(p) $$\n",
    "\n",
    "This means that the given $$r_i$$ is an equivalent of a value from the Bernoulli distribution with a probability of *p* for this value to be *1*. If *r_i* is a single value from this distribution, a draw from this distribution, reshaped to match the layer output, can be used as mask to these outputs.\n",
    "\n",
    "We are returned an array filled with values of *1* with a probability of *p* and otherwise value of *0*. We then apply this filter to the output of a layer we want to add dropout to.\n",
    "\n",
    "With the code, we have one hyperparameter for a dropout layer. This is a value for the percentage of neurons to disable in that layer. For example, if we choose 0.10 for the dropout parameter, 10% of the neuron will be disabled at random during each forward pass.\n",
    "\n"
   ],
   "id": "258cd2f3b188df84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T04:11:54.034021Z",
     "start_time": "2025-04-27T04:11:54.023283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Example output containing 10 values\n",
    "example_output = [0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73]\n",
    "\n",
    "# Repeat as long as necessary\n",
    "while True:\n",
    "\n",
    "    # Randomly choose index and set value to 0\n",
    "    index = random.randint(0, len(example_output) - 1)\n",
    "    example_output[index] = 0\n",
    "\n",
    "    # We might set an index that already is zeroed\n",
    "    # There are different ways of overcoming this problem,\n",
    "    # for simplicity we count values that exactly 0\n",
    "    # while it's extremely rare in real model that weights\n",
    "    # are exactly 0, this is not the best method for sure\n",
    "    dropped_out = 0\n",
    "    for value in example_output:\n",
    "        if value == 0:\n",
    "            dropped_out += 1\n",
    "\n",
    "    # If required number of outputs is zeroed - leave the loop\n",
    "    if dropped_out / len(example_output) >= dropout_rate:\n",
    "        break\n",
    "\n",
    "print(example_output)"
   ],
   "id": "9567f116561e0e27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27, 0, 0.67, 0, 0.05, 0, -2.01, 0, 0, 0.73]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A binomial distribution differs from Bernoulli distribution in one way, as it adds a parameter, *n*, which is the number of concurrent experiments (instead of just one) and returns the number of successes from the these *n* experiments.\n",
    "\n",
    "`np.random.binomial()` works by taking the already discussed parameter *n* (number of experiments) and *p* (probability of the true value of the experiment) as well as an additional parameter *size*:\n",
    "\n",
    "<code>\n",
    "\n",
    "np.random.binomial(n, p, size)\n",
    "\n",
    "</code>\n",
    "\n",
    "The function itself can be thought of like a coin toss, where the result will be 0 or 1. The *n* is how many tosses the coin we want to do. The *p* is the probability for the toss result to be 1. The overal result is a sum of all toss results. the *size* is how many of these \"test\" to run, and the return is a list of overall results."
   ],
   "id": "9c3bc574dc426f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T04:12:14.863761Z",
     "start_time": "2025-04-27T04:12:11.702418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.binomial(2, 0.5, size=10)"
   ],
   "id": "e21b3c2c8793155",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 0, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can use this to create a dropout layer. Our goal here is to create a filter where the intended dropout % is represented as 0, with everything else as 1. For example, let's say we have a dropout layer that we'll add after a layer that consists of 5 neurons, and we wish to have a 20% dropout. An example of a drop out layer might look like:\n",
    "\n",
    "[1, 0, 1, 1, 1]\n",
    "\n",
    "1/5 of that list is 0. This is an example of the filter we're going to apply to the output of the dense layer. If we multiplied a neural network's layer output by this, we'd be effectively disabling the neuron at the same index as the 0."
   ],
   "id": "a8263d6d362088be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T04:15:21.436701Z",
     "start_time": "2025-04-27T04:15:21.424942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dropout_rate = 0.20\n",
    "np.random.binomial(1, 1-dropout_rate, size=5)"
   ],
   "id": "a922aed19067b468",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is based on probabilities, so there will be times when it does not look like the above array. There could be times no neurons zero out, or all neurons zero out. On average, these random draws will tend toward the probability we desire. Also, this was an example using a very small layer (5 neurons). On a realistically sized layer, we should find the probability more consistenly matches our intedended value.",
   "id": "8a272da865315195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T04:39:29.122327Z",
     "start_time": "2025-04-27T04:39:29.111533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05,\n",
    "                            -0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "\n",
    "dropout_rate = 0.30\n",
    "example_output *= np.random.binomial(1, 1-dropout_rate, example_output.shape)\n",
    "\n",
    "example_output"
   ],
   "id": "d63b846d9d951cac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27, -1.03,  0.67,  0.  ,  0.05, -0.37, -2.01,  1.13, -0.07,\n",
       "        0.73])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While dropout helps a neural network generalize and is helpful for training, it's not something we want to utilize when predicting. It's not as simple as only omitting it becayse the magnitude of inputs to the next neurons can be dramatically different. If we have a dropout of 50%, for example, this would suggest that, on average, our inputs to the next layer neurons will be 50% smaller when summed, assuming they are fully-connected. What that means is that we used dropout during training, and, in this example, a random 50% of neuron output a value of 0 at each of the steps. Neurons in the next layer multiply inputs by weights, sum them, and receive values of 0 for half of their inputs. If we don't use dropout during prediction, all neurons will output their values, and this state won't match the state seen during training, since the sums will be statistically about twice as big. To handle this, during prediction, we might multiply all of the outputs by the dropout fraction, but that'd add another step for the forward pass, and there is a better way to achieve this. Instead, we want to scale the data back up after a dropout, during the training phase, to mimic the mean of the sum when all of the neurons output their values.\n",
    "*example_output* becomes:"
   ],
   "id": "aa87b5db1864e18b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T04:39:32.677443Z",
     "start_time": "2025-04-27T04:39:32.669379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_output *= np.random.binomial(1, 1-dropout_rate, example_output.shape) / (1 - dropout_rate)\n",
    "print(example_output)"
   ],
   "id": "6357dcd087d4f717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -1.47142857  0.95714286  0.          0.         -0.52857143\n",
      " -0.          1.61428571 -0.1         1.04285714]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice that we added a division of the dropout's result by the dropout rate. Since this rate is a fraction, it makes the resulting values larger, accounting for the value lost because a fraction of the neuron outputs being zeroed out. This way, we don't have to worry about the prediction and can simply omit the dropout during prediction. In any specific example, we will find that scaling doesn't equal the same sum as before because we're randomly dropping neurons. That said, after enough samples, the scaling will average out overall. to prove this:",
   "id": "16d68102c58e1dd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T04:36:15.811058Z",
     "start_time": "2025-04-27T04:36:04.635746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dropout_rate = 0.2\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05,\n",
    "                        -0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "\n",
    "print(f\"sum initial {sum(example_output)}\")\n",
    "\n",
    "sums = []\n",
    "\n",
    "for i in range(1000000):\n",
    "    example_output2 = example_output * np.random.binomial(1, 1-dropout_rate, example_output.shape) / (1 - dropout_rate)\n",
    "    sums.append(sum(example_output2))\n",
    "\n",
    "print(f\"mean sum: {np.mean(sums)}\")"
   ],
   "id": "8639f42bfd48d917",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum initial 0.36000000000000015\n",
      "mean sum: 0.35974747500000015\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Backward Pass\n",
    "\n",
    "When the value of element r_i\n",
    " equals 1, its function and derivative becomes the neuronâ€™s output, z,\n",
    "compensated for the loss value by 1-q, where q is the dropout rate,\n"
   ],
   "id": "acca825b8aeb6d9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T07:02:33.427634Z",
     "start_time": "2025-04-27T07:02:33.414983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer_Dropout:\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1,\n",
    "                                              self.rate,\n",
    "                                              size=inputs.shape) / \\\n",
    "                            self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ],
   "id": "f8ba3fd1b2328b3e",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Full code up to now",
   "id": "a4fdcff82a39b45f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T07:03:30.578747Z",
     "start_time": "2025-04-27T07:03:30.454281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ],
   "id": "4d6ac43bed3c82d7",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T07:22:26.244352Z",
     "start_time": "2025-04-27T07:22:26.230027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer_Dense:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                 weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0,\n",
    "                 bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues,\n",
    "                              axis=0,\n",
    "                              keepdims=True)\n",
    "\n",
    "        # gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ],
   "id": "75df6f9d6872eb48",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:39.603232Z",
     "start_time": "2025-04-30T06:24:39.595240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer_Dropout:\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1,\n",
    "                                              self.rate,\n",
    "                                              size=inputs.shape) /  self.rate\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ],
   "id": "44ce40d6cc379894",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:40.198845Z",
     "start_time": "2025-04-30T06:24:40.189942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ],
   "id": "7365876a57e9748",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:41.044630Z",
     "start_time": "2025-04-30T06:24:41.035249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        exp_values = np.exp(inputs - np.max(inputs,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalue) in enumerate(zip(self.output, dvalues)):\n",
    "            # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                                np.dot(single_output, single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalue)"
   ],
   "id": "8a2f9d6d54cf0574",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:41.702047Z",
     "start_time": "2025-04-30T06:24:41.690356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=1.,\n",
    "                 decay=0.,\n",
    "                 momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iteration = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning * \\\n",
    "                                         (1. / (1. + self.decay * self.interations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if self.momentum:\n",
    "\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                            self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                            self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ],
   "id": "237a126b224fabc7",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:42.194120Z",
     "start_time": "2025-04-30T06:24:42.182708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_Adagrad:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=1.,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                         (1. / (1. + self.decay * self.iterations))\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update caches with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "\n",
    "        # Vanilla SGD paramter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                        layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiaes / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ],
   "id": "4009f8f641104b2c",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:42.839999Z",
     "start_time": "2025-04-30T06:24:42.828065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_RMSProp:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                         (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                             (1 - self.rho) * layer.dbiases**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                           (1 - self.rho) * layer.biases**2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                        layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ],
   "id": "11cbbc62b146773a",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:43.341348Z",
     "start_time": "2025-04-30T06:24:43.328563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_Adam:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                         (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                    layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                                layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "                             (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "                           (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                        weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) + self. epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        bias_momentums_corrected / \\\n",
    "                        (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "f95d34c314892c9",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:43.916382Z",
     "start_time": "2025-04-30T06:24:43.906640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss:\n",
    "\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        regularization_loss = 0\n",
    "\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.weights))\n",
    "\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                   (np.sum(np.abs(layer.biases)))\n",
    "\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                    np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n"
   ],
   "id": "16dadcb26b946aaa",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:44.444451Z",
     "start_time": "2025-04-30T06:24:44.434233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n"
   ],
   "id": "884ab2c42a9e2a57",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:24:45.081285Z",
     "start_time": "2025-04-30T06:24:45.073013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "\n",
    "        self.dinputs = self.dinputs / samples"
   ],
   "id": "55c8b59953bf0a0",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:40:00.857631Z",
     "start_time": "2025-04-30T06:38:32.092827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 512,\n",
    "                     weight_regularizer_l2=1e-4,\n",
    "                     bias_regularizer_l2=1e-5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "\n",
    "dense2 = Layer_Dense(512, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.001,\n",
    "                           decay=5e-5)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dropout1.forward(activation1.output)\n",
    "    dense2.forward(dropout1.output)\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'data_loss: {data_loss}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ],
   "id": "726dc9d7b12b6311",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.35, loss: 5.606, data_loss: 5.497826099395752, reg_loss: 0.109, lr: 0.001\n",
      "epoch: 100, acc: 0.35, loss: 2.489, data_loss: 2.3822672367095947, reg_loss: 0.107, lr: 0.0009950743818100403\n",
      "epoch: 200, acc: 0.43666666666666665, loss: 2.126, data_loss: 2.021054983139038, reg_loss: 0.105, lr: 0.000990148027130056\n",
      "epoch: 300, acc: 0.44, loss: 2.305, data_loss: 2.2016849517822266, reg_loss: 0.103, lr: 0.00098527021035519\n",
      "epoch: 400, acc: 0.4633333333333333, loss: 2.315, data_loss: 2.213624954223633, reg_loss: 0.101, lr: 0.0009804402176577284\n",
      "epoch: 500, acc: 0.5033333333333333, loss: 2.177, data_loss: 2.0772554874420166, reg_loss: 0.099, lr: 0.0009756573491389824\n",
      "epoch: 600, acc: 0.4866666666666667, loss: 1.849, data_loss: 1.751529574394226, reg_loss: 0.097, lr: 0.000970920918491189\n",
      "epoch: 700, acc: 0.45666666666666667, loss: 1.997, data_loss: 1.9017854928970337, reg_loss: 0.095, lr: 0.0009662302526692111\n",
      "epoch: 800, acc: 0.5366666666666666, loss: 1.622, data_loss: 1.5291496515274048, reg_loss: 0.093, lr: 0.0009615846915717103\n",
      "epoch: 900, acc: 0.5466666666666666, loss: 1.636, data_loss: 1.544340968132019, reg_loss: 0.091, lr: 0.0009569835877314704\n",
      "epoch: 1000, acc: 0.53, loss: 1.536, data_loss: 1.446071743965149, reg_loss: 0.090, lr: 0.0009524263060145722\n",
      "epoch: 1100, acc: 0.56, loss: 1.425, data_loss: 1.3369508981704712, reg_loss: 0.088, lr: 0.0009479122233281198\n",
      "epoch: 1200, acc: 0.5433333333333333, loss: 1.450, data_loss: 1.3640186786651611, reg_loss: 0.086, lr: 0.0009434407283362424\n",
      "epoch: 1300, acc: 0.5733333333333334, loss: 1.487, data_loss: 1.4024754762649536, reg_loss: 0.084, lr: 0.0009390112211840931\n",
      "epoch: 1400, acc: 0.49666666666666665, loss: 1.661, data_loss: 1.5776090621948242, reg_loss: 0.083, lr: 0.0009346231132295903\n",
      "epoch: 1500, acc: 0.5666666666666667, loss: 1.294, data_loss: 1.2122105360031128, reg_loss: 0.081, lr: 0.000930275826782641\n",
      "epoch: 1600, acc: 0.58, loss: 1.342, data_loss: 1.2619516849517822, reg_loss: 0.080, lr: 0.0009259687948516136\n",
      "epoch: 1700, acc: 0.58, loss: 1.312, data_loss: 1.2338675260543823, reg_loss: 0.079, lr: 0.0009217014608968155\n",
      "epoch: 1800, acc: 0.6166666666666667, loss: 1.088, data_loss: 1.0106884241104126, reg_loss: 0.077, lr: 0.0009174732785907612\n",
      "epoch: 1900, acc: 0.61, loss: 1.208, data_loss: 1.1319410800933838, reg_loss: 0.076, lr: 0.0009132837115850038\n",
      "epoch: 2000, acc: 0.6133333333333333, loss: 1.219, data_loss: 1.1445945501327515, reg_loss: 0.075, lr: 0.000909132233283331\n",
      "epoch: 2100, acc: 0.65, loss: 1.053, data_loss: 0.9792798757553101, reg_loss: 0.073, lr: 0.000905018326621114\n",
      "epoch: 2200, acc: 0.6633333333333333, loss: 1.056, data_loss: 0.9842315912246704, reg_loss: 0.072, lr: 0.0009009414838506239\n",
      "epoch: 2300, acc: 0.6233333333333333, loss: 1.087, data_loss: 1.0159125328063965, reg_loss: 0.071, lr: 0.0008969012063321225\n",
      "epoch: 2400, acc: 0.71, loss: 0.955, data_loss: 0.8852648735046387, reg_loss: 0.070, lr: 0.0008928970043305506\n",
      "epoch: 2500, acc: 0.6433333333333333, loss: 1.095, data_loss: 1.0262060165405273, reg_loss: 0.069, lr: 0.0008889283968176362\n",
      "epoch: 2600, acc: 0.6366666666666667, loss: 1.011, data_loss: 0.9431495070457458, reg_loss: 0.068, lr: 0.0008849949112792601\n",
      "epoch: 2700, acc: 0.66, loss: 1.002, data_loss: 0.9345930814743042, reg_loss: 0.067, lr: 0.0008810960835279088\n",
      "epoch: 2800, acc: 0.67, loss: 0.951, data_loss: 0.8850433230400085, reg_loss: 0.066, lr: 0.0008772314575200668\n",
      "epoch: 2900, acc: 0.6833333333333333, loss: 0.926, data_loss: 0.8608434200286865, reg_loss: 0.065, lr: 0.0008734005851783922\n",
      "epoch: 3000, acc: 0.66, loss: 1.025, data_loss: 0.9606383442878723, reg_loss: 0.064, lr: 0.0008696030262185313\n",
      "epoch: 3100, acc: 0.73, loss: 0.900, data_loss: 0.8365573287010193, reg_loss: 0.063, lr: 0.0008658383479804322\n",
      "epoch: 3200, acc: 0.6933333333333334, loss: 0.926, data_loss: 0.8635757565498352, reg_loss: 0.062, lr: 0.00086210612526402\n",
      "epoch: 3300, acc: 0.6666666666666666, loss: 0.890, data_loss: 0.8286709785461426, reg_loss: 0.062, lr: 0.000858405940169106\n",
      "epoch: 3400, acc: 0.73, loss: 0.817, data_loss: 0.7559908032417297, reg_loss: 0.061, lr: 0.0008547373819393991\n",
      "epoch: 3500, acc: 0.74, loss: 0.795, data_loss: 0.7348060011863708, reg_loss: 0.060, lr: 0.0008511000468105027\n",
      "epoch: 3600, acc: 0.7066666666666667, loss: 0.845, data_loss: 0.7855318188667297, reg_loss: 0.059, lr: 0.0008474935378617737\n",
      "epoch: 3700, acc: 0.7366666666666667, loss: 0.813, data_loss: 0.7539694309234619, reg_loss: 0.059, lr: 0.0008439174648719356\n",
      "epoch: 3800, acc: 0.7133333333333334, loss: 0.786, data_loss: 0.7277233004570007, reg_loss: 0.058, lr: 0.0008403714441783267\n",
      "epoch: 3900, acc: 0.73, loss: 0.838, data_loss: 0.7810457944869995, reg_loss: 0.057, lr: 0.0008368550985396879\n",
      "epoch: 4000, acc: 0.7566666666666667, loss: 0.764, data_loss: 0.7076963186264038, reg_loss: 0.057, lr: 0.0008333680570023751\n",
      "epoch: 4100, acc: 0.7366666666666667, loss: 0.744, data_loss: 0.6876463294029236, reg_loss: 0.056, lr: 0.0008299099547699075\n",
      "epoch: 4200, acc: 0.7333333333333333, loss: 0.795, data_loss: 0.7398499846458435, reg_loss: 0.056, lr: 0.0008264804330757469\n",
      "epoch: 4300, acc: 0.7466666666666667, loss: 0.719, data_loss: 0.6636651158332825, reg_loss: 0.055, lr: 0.0008230791390592206\n",
      "epoch: 4400, acc: 0.7466666666666667, loss: 0.791, data_loss: 0.7364242076873779, reg_loss: 0.054, lr: 0.0008197057256444936\n",
      "epoch: 4500, acc: 0.79, loss: 0.653, data_loss: 0.598856508731842, reg_loss: 0.054, lr: 0.000816359851422507\n",
      "epoch: 4600, acc: 0.79, loss: 0.692, data_loss: 0.6389825940132141, reg_loss: 0.053, lr: 0.0008130411805357941\n",
      "epoch: 4700, acc: 0.78, loss: 0.688, data_loss: 0.6351500749588013, reg_loss: 0.053, lr: 0.0008097493825660958\n",
      "epoch: 4800, acc: 0.7733333333333333, loss: 0.722, data_loss: 0.6696922779083252, reg_loss: 0.052, lr: 0.0008064841324246946\n",
      "epoch: 4900, acc: 0.79, loss: 0.658, data_loss: 0.6064130663871765, reg_loss: 0.052, lr: 0.0008032451102453913\n",
      "epoch: 5000, acc: 0.7866666666666666, loss: 0.674, data_loss: 0.6225798726081848, reg_loss: 0.051, lr: 0.0008000320012800512\n",
      "epoch: 5100, acc: 0.76, loss: 0.747, data_loss: 0.6956575512886047, reg_loss: 0.051, lr: 0.0007968444957966453\n",
      "epoch: 5200, acc: 0.7666666666666667, loss: 0.689, data_loss: 0.6385861039161682, reg_loss: 0.050, lr: 0.0007936822889797215\n",
      "epoch: 5300, acc: 0.8166666666666667, loss: 0.636, data_loss: 0.5861242413520813, reg_loss: 0.050, lr: 0.0007905450808332346\n",
      "epoch: 5400, acc: 0.81, loss: 0.640, data_loss: 0.5905853509902954, reg_loss: 0.050, lr: 0.0007874325760856727\n",
      "epoch: 5500, acc: 0.79, loss: 0.651, data_loss: 0.601428210735321, reg_loss: 0.049, lr: 0.0007843444840974156\n",
      "epoch: 5600, acc: 0.7966666666666666, loss: 0.625, data_loss: 0.5761855840682983, reg_loss: 0.049, lr: 0.0007812805187702646\n",
      "epoch: 5700, acc: 0.7866666666666666, loss: 0.651, data_loss: 0.6026325821876526, reg_loss: 0.048, lr: 0.000778240398459084\n",
      "epoch: 5800, acc: 0.8066666666666666, loss: 0.608, data_loss: 0.5600013732910156, reg_loss: 0.048, lr: 0.0007752238458854994\n",
      "epoch: 5900, acc: 0.8066666666666666, loss: 0.628, data_loss: 0.5802937746047974, reg_loss: 0.048, lr: 0.0007722305880535929\n",
      "epoch: 6000, acc: 0.8166666666666667, loss: 0.595, data_loss: 0.5478725433349609, reg_loss: 0.047, lr: 0.000769260356167545\n",
      "epoch: 6100, acc: 0.8066666666666666, loss: 0.608, data_loss: 0.5607280731201172, reg_loss: 0.047, lr: 0.0007663128855511705\n",
      "epoch: 6200, acc: 0.8266666666666667, loss: 0.580, data_loss: 0.53290855884552, reg_loss: 0.047, lr: 0.0007633879155692965\n",
      "epoch: 6300, acc: 0.7966666666666666, loss: 0.620, data_loss: 0.5733023285865784, reg_loss: 0.046, lr: 0.0007604851895509335\n",
      "epoch: 6400, acc: 0.84, loss: 0.613, data_loss: 0.5669673085212708, reg_loss: 0.046, lr: 0.0007576044547141939\n",
      "epoch: 6500, acc: 0.81, loss: 0.594, data_loss: 0.5486440062522888, reg_loss: 0.046, lr: 0.0007547454620929092\n",
      "epoch: 6600, acc: 0.8066666666666666, loss: 0.609, data_loss: 0.5633699297904968, reg_loss: 0.045, lr: 0.0007519079664649048\n",
      "epoch: 6700, acc: 0.8466666666666667, loss: 0.541, data_loss: 0.4954882264137268, reg_loss: 0.045, lr: 0.0007490917262818832\n",
      "epoch: 6800, acc: 0.8033333333333333, loss: 0.612, data_loss: 0.567030668258667, reg_loss: 0.045, lr: 0.0007462965036008806\n",
      "epoch: 6900, acc: 0.8166666666666667, loss: 0.565, data_loss: 0.5201388597488403, reg_loss: 0.044, lr: 0.0007435220640172497\n",
      "epoch: 7000, acc: 0.82, loss: 0.564, data_loss: 0.5195769667625427, reg_loss: 0.044, lr: 0.0007407681765991334\n",
      "epoch: 7100, acc: 0.8, loss: 0.596, data_loss: 0.5518835186958313, reg_loss: 0.044, lr: 0.0007380346138233883\n",
      "epoch: 7200, acc: 0.81, loss: 0.605, data_loss: 0.5611809492111206, reg_loss: 0.044, lr: 0.0007353211515129233\n",
      "epoch: 7300, acc: 0.8166666666666667, loss: 0.536, data_loss: 0.49230194091796875, reg_loss: 0.043, lr: 0.0007326275687754132\n",
      "epoch: 7400, acc: 0.8166666666666667, loss: 0.594, data_loss: 0.550967276096344, reg_loss: 0.043, lr: 0.0007299536479433556\n",
      "epoch: 7500, acc: 0.84, loss: 0.548, data_loss: 0.5051096081733704, reg_loss: 0.043, lr: 0.000727299174515437\n",
      "epoch: 7600, acc: 0.8466666666666667, loss: 0.569, data_loss: 0.5264509320259094, reg_loss: 0.043, lr: 0.0007246639370991702\n",
      "epoch: 7700, acc: 0.83, loss: 0.582, data_loss: 0.5397971272468567, reg_loss: 0.043, lr: 0.0007220477273547782\n",
      "epoch: 7800, acc: 0.8166666666666667, loss: 0.573, data_loss: 0.5308613777160645, reg_loss: 0.042, lr: 0.0007194503399402856\n",
      "epoch: 7900, acc: 0.8433333333333334, loss: 0.525, data_loss: 0.48302632570266724, reg_loss: 0.042, lr: 0.0007168715724577941\n",
      "epoch: 8000, acc: 0.8133333333333334, loss: 0.544, data_loss: 0.5015724301338196, reg_loss: 0.042, lr: 0.0007143112254009072\n",
      "epoch: 8100, acc: 0.8133333333333334, loss: 0.558, data_loss: 0.5159923434257507, reg_loss: 0.042, lr: 0.0007117691021032778\n",
      "epoch: 8200, acc: 0.8333333333333334, loss: 0.552, data_loss: 0.5098309516906738, reg_loss: 0.042, lr: 0.0007092450086882513\n",
      "epoch: 8300, acc: 0.82, loss: 0.531, data_loss: 0.48931753635406494, reg_loss: 0.042, lr: 0.0007067387540195766\n",
      "epoch: 8400, acc: 0.83, loss: 0.532, data_loss: 0.49039652943611145, reg_loss: 0.041, lr: 0.0007042501496531569\n",
      "epoch: 8500, acc: 0.8433333333333334, loss: 0.560, data_loss: 0.5186891555786133, reg_loss: 0.041, lr: 0.0007017790097898172\n",
      "epoch: 8600, acc: 0.83, loss: 0.566, data_loss: 0.5252957940101624, reg_loss: 0.041, lr: 0.0006993251512290639\n",
      "epoch: 8700, acc: 0.83, loss: 0.542, data_loss: 0.5014215111732483, reg_loss: 0.041, lr: 0.0006968883933238093\n",
      "epoch: 8800, acc: 0.84, loss: 0.519, data_loss: 0.4784619212150574, reg_loss: 0.041, lr: 0.0006944685579360394\n",
      "epoch: 8900, acc: 0.8433333333333334, loss: 0.525, data_loss: 0.4841194152832031, reg_loss: 0.041, lr: 0.0006920654693934046\n",
      "epoch: 9000, acc: 0.8433333333333334, loss: 0.548, data_loss: 0.5074859857559204, reg_loss: 0.041, lr: 0.000689678954446705\n",
      "epoch: 9100, acc: 0.83, loss: 0.531, data_loss: 0.4906177222728729, reg_loss: 0.041, lr: 0.0006873088422282553\n",
      "epoch: 9200, acc: 0.8633333333333333, loss: 0.519, data_loss: 0.47901204228401184, reg_loss: 0.040, lr: 0.0006849549642111032\n",
      "epoch: 9300, acc: 0.83, loss: 0.527, data_loss: 0.48699629306793213, reg_loss: 0.040, lr: 0.0006826171541690843\n",
      "epoch: 9400, acc: 0.8366666666666667, loss: 0.521, data_loss: 0.4805823266506195, reg_loss: 0.040, lr: 0.0006802952481376917\n",
      "epoch: 9500, acc: 0.8366666666666667, loss: 0.520, data_loss: 0.47982484102249146, reg_loss: 0.040, lr: 0.0006779890843757416\n",
      "epoch: 9600, acc: 0.85, loss: 0.517, data_loss: 0.4768427014350891, reg_loss: 0.040, lr: 0.0006756985033278152\n",
      "epoch: 9700, acc: 0.84, loss: 0.525, data_loss: 0.48559972643852234, reg_loss: 0.040, lr: 0.0006734233475874608\n",
      "epoch: 9800, acc: 0.8433333333333334, loss: 0.517, data_loss: 0.47762635350227356, reg_loss: 0.040, lr: 0.0006711634618611363\n",
      "epoch: 9900, acc: 0.8533333333333334, loss: 0.500, data_loss: 0.46047964692115784, reg_loss: 0.040, lr: 0.000668918692932874\n",
      "epoch: 10000, acc: 0.87, loss: 0.481, data_loss: 0.4416980445384979, reg_loss: 0.040, lr: 0.0006666888896296543\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T06:45:45.319187Z",
     "start_time": "2025-04-30T06:45:45.305053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n"
   ],
   "id": "8140b73d602c6f65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.737, loss: 0.597\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a8bddd91f35a819c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
