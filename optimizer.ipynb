{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T00:58:40.451001Z",
     "start_time": "2025-04-06T00:58:40.424738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # store the inputs for gradient calculation later\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Calculate gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T,\n",
    "                               dvalues)\n",
    "        self.dbiases = np.sum(dvalues,\n",
    "                             axis=0,\n",
    "                             keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU Activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variables, let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradients where input values were negative or zero\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax Activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalzed probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Common Loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculate the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean toward any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only if one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Number of labels in every sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradients\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "\n",
    "    # Create activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "\n",
    "        # set the output\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encode, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,\n",
    "                               axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ],
   "id": "f0f09af42921f0c5",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T00:58:41.489501Z",
     "start_time": "2025-04-06T00:58:41.475578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Calculate accuracy from output of loss_activation and targets along first axis\n",
    "predictions = np.argmax(loss_activation.output,\n",
    "                        axis=1)\n",
    "\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ],
   "id": "5905604e1337782d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T00:58:42.278392Z",
     "start_time": "2025-04-06T00:58:42.270884Z"
    }
   },
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T00:58:43.546792Z",
     "start_time": "2025-04-06T00:58:43.539902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an optimizer object\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "# Update our network layer's parameters after calculating the gradient using:\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ],
   "id": "d6bcf4e1c53c8005",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The layer object contains its parameters (weights and biases) and also, at this stage, the gradient that is calculated during backpropagation. We store these in the layer's properties so that the optimizer can make use of them. In our main neural network code, we'd bring the optimization in after backpropagation. Let's make a 1x64 densely connected neural network (1 hidden layer with 64 neurons) and use the same dataset as before:\n",
   "id": "f14ffe68f0ed6e8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T00:58:45.835432Z",
     "start_time": "2025-04-06T00:58:45.816273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "print(f'loss: {loss}')\n",
    "\n",
    "# Calculate accuracy\n",
    "prediction = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print(f'acc: {accuracy}')\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "# Use optimizer to update weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)\n"
   ],
   "id": "ebf3fc4b03b100bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.098605751991272\n",
      "acc: 0.34\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is everything we need to train out model. We will repeatedly perform a forward pass, backward pass, and optimization until we reach some stopping point. Each full pass through all of the training data is called an **epoch**. In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to have a perfect model with ideal weights and biases afer only one epoch.",
   "id": "2b17525f8d3e6e54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T00:59:56.716094Z",
     "start_time": "2025-04-06T00:59:39.307700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}\")\n",
    "\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n"
   ],
   "id": "a24c9b31d2fa5386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.350, loss: 1.099\n",
      "epoch: 100, acc: 0.390, loss: 1.083\n",
      "epoch: 200, acc: 0.417, loss: 1.072\n",
      "epoch: 300, acc: 0.410, loss: 1.071\n",
      "epoch: 400, acc: 0.417, loss: 1.070\n",
      "epoch: 500, acc: 0.407, loss: 1.069\n",
      "epoch: 600, acc: 0.393, loss: 1.067\n",
      "epoch: 700, acc: 0.393, loss: 1.064\n",
      "epoch: 800, acc: 0.397, loss: 1.056\n",
      "epoch: 900, acc: 0.400, loss: 1.056\n",
      "epoch: 1000, acc: 0.397, loss: 1.049\n",
      "epoch: 1100, acc: 0.417, loss: 1.041\n",
      "epoch: 1200, acc: 0.443, loss: 1.035\n",
      "epoch: 1300, acc: 0.473, loss: 1.030\n",
      "epoch: 1400, acc: 0.500, loss: 1.023\n",
      "epoch: 1500, acc: 0.503, loss: 1.020\n",
      "epoch: 1600, acc: 0.380, loss: 1.012\n",
      "epoch: 1700, acc: 0.380, loss: 1.008\n",
      "epoch: 1800, acc: 0.453, loss: 1.006\n",
      "epoch: 1900, acc: 0.503, loss: 1.020\n",
      "epoch: 2000, acc: 0.493, loss: 1.026\n",
      "epoch: 2100, acc: 0.403, loss: 0.991\n",
      "epoch: 2200, acc: 0.423, loss: 1.038\n",
      "epoch: 2300, acc: 0.527, loss: 0.995\n",
      "epoch: 2400, acc: 0.457, loss: 0.981\n",
      "epoch: 2500, acc: 0.393, loss: 0.977\n",
      "epoch: 2600, acc: 0.523, loss: 0.984\n",
      "epoch: 2700, acc: 0.487, loss: 0.992\n",
      "epoch: 2800, acc: 0.430, loss: 0.967\n",
      "epoch: 2900, acc: 0.460, loss: 0.976\n",
      "epoch: 3000, acc: 0.500, loss: 0.987\n",
      "epoch: 3100, acc: 0.473, loss: 0.944\n",
      "epoch: 3200, acc: 0.470, loss: 0.976\n",
      "epoch: 3300, acc: 0.517, loss: 0.954\n",
      "epoch: 3400, acc: 0.560, loss: 0.964\n",
      "epoch: 3500, acc: 0.503, loss: 0.947\n",
      "epoch: 3600, acc: 0.513, loss: 0.929\n",
      "epoch: 3700, acc: 0.450, loss: 0.946\n",
      "epoch: 3800, acc: 0.560, loss: 0.938\n",
      "epoch: 3900, acc: 0.500, loss: 0.940\n",
      "epoch: 4000, acc: 0.523, loss: 0.916\n",
      "epoch: 4100, acc: 0.460, loss: 0.930\n",
      "epoch: 4200, acc: 0.563, loss: 0.924\n",
      "epoch: 4300, acc: 0.503, loss: 0.922\n",
      "epoch: 4400, acc: 0.527, loss: 0.903\n",
      "epoch: 4500, acc: 0.497, loss: 0.902\n",
      "epoch: 4600, acc: 0.580, loss: 0.916\n",
      "epoch: 4700, acc: 0.507, loss: 0.901\n",
      "epoch: 4800, acc: 0.503, loss: 0.884\n",
      "epoch: 4900, acc: 0.550, loss: 0.907\n",
      "epoch: 5000, acc: 0.540, loss: 0.884\n",
      "epoch: 5100, acc: 0.500, loss: 0.866\n",
      "epoch: 5200, acc: 0.487, loss: 0.953\n",
      "epoch: 5300, acc: 0.553, loss: 0.878\n",
      "epoch: 5400, acc: 0.517, loss: 0.895\n",
      "epoch: 5500, acc: 0.537, loss: 0.871\n",
      "epoch: 5600, acc: 0.593, loss: 0.867\n",
      "epoch: 5700, acc: 0.567, loss: 0.891\n",
      "epoch: 5800, acc: 0.550, loss: 0.874\n",
      "epoch: 5900, acc: 0.540, loss: 0.903\n",
      "epoch: 6000, acc: 0.587, loss: 0.843\n",
      "epoch: 6100, acc: 0.577, loss: 0.852\n",
      "epoch: 6200, acc: 0.537, loss: 0.839\n",
      "epoch: 6300, acc: 0.513, loss: 0.917\n",
      "epoch: 6400, acc: 0.620, loss: 0.804\n",
      "epoch: 6500, acc: 0.577, loss: 0.828\n",
      "epoch: 6600, acc: 0.583, loss: 0.872\n",
      "epoch: 6700, acc: 0.553, loss: 0.876\n",
      "epoch: 6800, acc: 0.497, loss: 0.942\n",
      "epoch: 6900, acc: 0.623, loss: 0.799\n",
      "epoch: 7000, acc: 0.557, loss: 0.805\n",
      "epoch: 7100, acc: 0.630, loss: 0.794\n",
      "epoch: 7200, acc: 0.570, loss: 0.807\n",
      "epoch: 7300, acc: 0.630, loss: 0.793\n",
      "epoch: 7400, acc: 0.577, loss: 0.805\n",
      "epoch: 7500, acc: 0.617, loss: 0.805\n",
      "epoch: 7600, acc: 0.460, loss: 1.000\n",
      "epoch: 7700, acc: 0.573, loss: 0.918\n",
      "epoch: 7800, acc: 0.523, loss: 0.875\n",
      "epoch: 7900, acc: 0.640, loss: 0.766\n",
      "epoch: 8000, acc: 0.593, loss: 0.867\n",
      "epoch: 8100, acc: 0.583, loss: 0.815\n",
      "epoch: 8200, acc: 0.630, loss: 0.781\n",
      "epoch: 8300, acc: 0.530, loss: 0.844\n",
      "epoch: 8400, acc: 0.553, loss: 0.946\n",
      "epoch: 8500, acc: 0.520, loss: 0.882\n",
      "epoch: 8600, acc: 0.647, loss: 0.763\n",
      "epoch: 8700, acc: 0.610, loss: 0.753\n",
      "epoch: 8800, acc: 0.590, loss: 0.818\n",
      "epoch: 8900, acc: 0.613, loss: 0.774\n",
      "epoch: 9000, acc: 0.533, loss: 0.879\n",
      "epoch: 9100, acc: 0.580, loss: 0.832\n",
      "epoch: 9200, acc: 0.633, loss: 0.783\n",
      "epoch: 9300, acc: 0.530, loss: 0.942\n",
      "epoch: 9400, acc: 0.613, loss: 0.808\n",
      "epoch: 9500, acc: 0.587, loss: 0.832\n",
      "epoch: 9600, acc: 0.593, loss: 0.819\n",
      "epoch: 9700, acc: 0.620, loss: 0.782\n",
      "epoch: 9800, acc: 0.483, loss: 1.160\n",
      "epoch: 9900, acc: 0.573, loss: 0.797\n",
      "epoch: 10000, acc: 0.660, loss: 0.748\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Learning Rate\n",
   "id": "f188ddd8ddcba857"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:55:05.855788Z",
     "start_time": "2025-04-06T20:54:58.769449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loos_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(learning_rate=0.85)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy}, loss: {loss}\")\n",
    "\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ],
   "id": "5a08ff96a110324c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.34, loss: 1.0986042022705078\n",
      "epoch: 100, acc: 0.42333333333333334, loss: 1.0861332416534424\n",
      "epoch: 200, acc: 0.41, loss: 1.0704448223114014\n",
      "epoch: 300, acc: 0.43, loss: 1.0670863389968872\n",
      "epoch: 400, acc: 0.42333333333333334, loss: 1.065380573272705\n",
      "epoch: 500, acc: 0.43, loss: 1.0625945329666138\n",
      "epoch: 600, acc: 0.44, loss: 1.0569745302200317\n",
      "epoch: 700, acc: 0.43, loss: 1.0473774671554565\n",
      "epoch: 800, acc: 0.43333333333333335, loss: 1.0337408781051636\n",
      "epoch: 900, acc: 0.3933333333333333, loss: 1.034419298171997\n",
      "epoch: 1000, acc: 0.39666666666666667, loss: 1.0229823589324951\n",
      "epoch: 1100, acc: 0.4166666666666667, loss: 1.0165765285491943\n",
      "epoch: 1200, acc: 0.43666666666666665, loss: 1.0082749128341675\n",
      "epoch: 1300, acc: 0.45666666666666667, loss: 1.000727891921997\n",
      "epoch: 1400, acc: 0.4666666666666667, loss: 0.9951295852661133\n",
      "epoch: 1500, acc: 0.48333333333333334, loss: 0.9894167184829712\n",
      "epoch: 1600, acc: 0.5, loss: 0.9815554022789001\n",
      "epoch: 1700, acc: 0.4633333333333333, loss: 1.0000336170196533\n",
      "epoch: 1800, acc: 0.4666666666666667, loss: 0.9748817682266235\n",
      "epoch: 1900, acc: 0.49333333333333335, loss: 0.9764375686645508\n",
      "epoch: 2000, acc: 0.5, loss: 0.9777241945266724\n",
      "epoch: 2100, acc: 0.4766666666666667, loss: 0.9637344479560852\n",
      "epoch: 2200, acc: 0.48, loss: 0.9689666628837585\n",
      "epoch: 2300, acc: 0.49333333333333335, loss: 0.9672309160232544\n",
      "epoch: 2400, acc: 0.5066666666666667, loss: 0.9543545246124268\n",
      "epoch: 2500, acc: 0.48333333333333334, loss: 0.9587320685386658\n",
      "epoch: 2600, acc: 0.51, loss: 0.9468966722488403\n",
      "epoch: 2700, acc: 0.49, loss: 0.9448063373565674\n",
      "epoch: 2800, acc: 0.5233333333333333, loss: 0.9338452219963074\n",
      "epoch: 2900, acc: 0.49666666666666665, loss: 0.9398344159126282\n",
      "epoch: 3000, acc: 0.5233333333333333, loss: 0.9231420755386353\n",
      "epoch: 3100, acc: 0.5266666666666666, loss: 0.90749591588974\n",
      "epoch: 3200, acc: 0.5033333333333333, loss: 0.8935986161231995\n",
      "epoch: 3300, acc: 0.5066666666666667, loss: 0.8910515308380127\n",
      "epoch: 3400, acc: 0.5733333333333334, loss: 0.8846867084503174\n",
      "epoch: 3500, acc: 0.5633333333333334, loss: 0.8989300727844238\n",
      "epoch: 3600, acc: 0.5133333333333333, loss: 0.8808258175849915\n",
      "epoch: 3700, acc: 0.51, loss: 0.8677376508712769\n",
      "epoch: 3800, acc: 0.5866666666666667, loss: 0.8596315383911133\n",
      "epoch: 3900, acc: 0.5466666666666666, loss: 0.8346459865570068\n",
      "epoch: 4000, acc: 0.59, loss: 0.8361872434616089\n",
      "epoch: 4100, acc: 0.58, loss: 0.8060739040374756\n",
      "epoch: 4200, acc: 0.59, loss: 0.8110020160675049\n",
      "epoch: 4300, acc: 0.5966666666666667, loss: 0.7952545881271362\n",
      "epoch: 4400, acc: 0.5833333333333334, loss: 0.7789406776428223\n",
      "epoch: 4500, acc: 0.6266666666666667, loss: 0.779633104801178\n",
      "epoch: 4600, acc: 0.6, loss: 0.7759405374526978\n",
      "epoch: 4700, acc: 0.58, loss: 0.8074609637260437\n",
      "epoch: 4800, acc: 0.5933333333333334, loss: 0.7730790972709656\n",
      "epoch: 4900, acc: 0.5933333333333334, loss: 0.7550475597381592\n",
      "epoch: 5000, acc: 0.6166666666666667, loss: 0.7554157972335815\n",
      "epoch: 5100, acc: 0.6066666666666667, loss: 0.7559365630149841\n",
      "epoch: 5200, acc: 0.6166666666666667, loss: 0.7633763551712036\n",
      "epoch: 5300, acc: 0.6033333333333334, loss: 0.7697721123695374\n",
      "epoch: 5400, acc: 0.6566666666666666, loss: 0.7437585592269897\n",
      "epoch: 5500, acc: 0.61, loss: 0.725995659828186\n",
      "epoch: 5600, acc: 0.59, loss: 0.7259078025817871\n",
      "epoch: 5700, acc: 0.5966666666666667, loss: 0.7766896486282349\n",
      "epoch: 5800, acc: 0.6466666666666666, loss: 0.7498157024383545\n",
      "epoch: 5900, acc: 0.6233333333333333, loss: 0.7051754593849182\n",
      "epoch: 6000, acc: 0.64, loss: 0.7252432703971863\n",
      "epoch: 6100, acc: 0.64, loss: 0.6952326893806458\n",
      "epoch: 6200, acc: 0.6566666666666666, loss: 0.7683734893798828\n",
      "epoch: 6300, acc: 0.6566666666666666, loss: 0.7639535665512085\n",
      "epoch: 6400, acc: 0.65, loss: 0.6900726556777954\n",
      "epoch: 6500, acc: 0.6633333333333333, loss: 0.6998364329338074\n",
      "epoch: 6600, acc: 0.5966666666666667, loss: 0.7030099034309387\n",
      "epoch: 6700, acc: 0.68, loss: 0.6636685132980347\n",
      "epoch: 6800, acc: 0.6866666666666666, loss: 0.6538305878639221\n",
      "epoch: 6900, acc: 0.71, loss: 0.6793526411056519\n",
      "epoch: 7000, acc: 0.69, loss: 0.6442235112190247\n",
      "epoch: 7100, acc: 0.6766666666666666, loss: 0.642257571220398\n",
      "epoch: 7200, acc: 0.7033333333333334, loss: 0.643506646156311\n",
      "epoch: 7300, acc: 0.6533333333333333, loss: 0.6697818040847778\n",
      "epoch: 7400, acc: 0.69, loss: 0.643399178981781\n",
      "epoch: 7500, acc: 0.7033333333333334, loss: 0.630927324295044\n",
      "epoch: 7600, acc: 0.6466666666666666, loss: 0.6943070292472839\n",
      "epoch: 7700, acc: 0.7, loss: 0.6489564776420593\n",
      "epoch: 7800, acc: 0.69, loss: 0.6443862915039062\n",
      "epoch: 7900, acc: 0.6866666666666666, loss: 0.6812838912010193\n",
      "epoch: 8000, acc: 0.7, loss: 0.6272071599960327\n",
      "epoch: 8100, acc: 0.7066666666666667, loss: 0.6157978177070618\n",
      "epoch: 8200, acc: 0.6933333333333334, loss: 0.6401334404945374\n",
      "epoch: 8300, acc: 0.7133333333333334, loss: 0.6487255096435547\n",
      "epoch: 8400, acc: 0.7133333333333334, loss: 0.6173613667488098\n",
      "epoch: 8500, acc: 0.7066666666666667, loss: 0.6411105990409851\n",
      "epoch: 8600, acc: 0.71, loss: 0.6060052514076233\n",
      "epoch: 8700, acc: 0.69, loss: 0.5870285630226135\n",
      "epoch: 8800, acc: 0.7266666666666667, loss: 0.5911646485328674\n",
      "epoch: 8900, acc: 0.7333333333333333, loss: 0.5976299047470093\n",
      "epoch: 9000, acc: 0.6766666666666666, loss: 0.813940703868866\n",
      "epoch: 9100, acc: 0.7166666666666667, loss: 0.5739046931266785\n",
      "epoch: 9200, acc: 0.7166666666666667, loss: 0.5705629587173462\n",
      "epoch: 9300, acc: 0.75, loss: 0.578158438205719\n",
      "epoch: 9400, acc: 0.73, loss: 0.5588781833648682\n",
      "epoch: 9500, acc: 0.73, loss: 0.5637685656547546\n",
      "epoch: 9600, acc: 0.5866666666666667, loss: 0.9382584691047668\n",
      "epoch: 9700, acc: 0.72, loss: 0.5963098406791687\n",
      "epoch: 9800, acc: 0.7466666666666667, loss: 0.5410895347595215\n",
      "epoch: 9900, acc: 0.7266666666666667, loss: 0.5407727956771851\n",
      "epoch: 10000, acc: 0.7266666666666667, loss: 0.5505494475364685\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As can be seen, the neural network did slightly better in terms of accuracy, and it achieved a lower loss; lower loss is not always associated with higher accuracy. Even if we desire the best accuracy out of our model, the optimizer's task is to decrease loss, not raise accuracy directly. Loss is the mean value of all of the smaple losses, and some of them could drop significantly, while others might rise just slightly, chaning the prediction for them from a correct to an incorrect class at the same time. This would cause a lower mean loss in general, but also more incorrectly predicted samples, which will, at the same time, lower the accuracy. A likely reason for this model's lower accuracy is that it found andother local minimum by chance - the descent path has changed, due to smaller steps. In a direct comparison of these two models in training, different learning rates did not show that the lower this learning rate value is, the better. In most cases, we want to start with a larger learning rate and decrease the learning rate over time/steps.\n",
    "\n",
    "A commonly-used solution to keep initial updates large and explore various learning rates during training is to implement a **learning rate decay**.\n"
   ],
   "id": "e20e9c45b2eafd62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "The idea of a **learning rate decay** is to start with large learning rate, say 1.0 in our case, and then decrease it during training. There are a few methods for doing this. One is to decrease the learning rate in response to the loss across epochs - for example, if th eloss begins to level out/ plateau or starts \"jumping\" over large deltas. We can either program this behavior-monitoring logically or simply track our loss over time and manually decrease the learning rate when we deem it appropriate. Another option, which we will implement, is to program a **Decay Rate**, which steadily decays the learning rate per batch or epoch.\n",
    "\n",
    "Let's plan to decay per step. This can also be referred to as **1/t decaying** or **exponential decaying**. Basically, we're going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper-parameter that we'll add to the optimizer, called the **learning rate decay**. How this decaying works is it takes the step and the decaying ratio and multiplies them. The further in training, the bigger the step is, and the bigger result of this multiplication is. We then take its reciprocal (the firther in training, the lower the value) and multiply the initial learning rate by it. The added *1* makes sure that the resulting algorithm never raises the learning rate. for example, for the first step, we might divide 1 by the learning rate, *0.001* for example, which will result in a current learning rate of *1000*. That's definitely not what we wanted. 1 divided by the 1+fraction ensures that the result, a fraction of the starting learning rate, will always be less than or equals to 1, decreasing over time. That's the desired result -- start with the current learning rate and make smaller with time."
   ],
   "id": "9855961667dfa0e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T02:01:54.181192Z",
     "start_time": "2025-04-07T02:01:54.147577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "step = 1\n",
    "\n",
    "learning_rate = starting_learning_rate * (1. / (1. + learning_rate_decay * step))\n",
    "learning_rate"
   ],
   "id": "2eb20b6577cbff1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T02:03:55.960072Z",
     "start_time": "2025-04-07T02:03:55.951191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for step in range(20):\n",
    "    learning_rate = starting_learning_rate * (1. / (1. + learning_rate_decay * step))\n",
    "    print(learning_rate)"
   ],
   "id": "c26c6b7c187c3cdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9090909090909091\n",
      "0.8333333333333334\n",
      "0.7692307692307692\n",
      "0.7142857142857143\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.588235294117647\n",
      "0.5555555555555556\n",
      "0.5263157894736842\n",
      "0.5\n",
      "0.47619047619047616\n",
      "0.45454545454545453\n",
      "0.4347826086956522\n",
      "0.41666666666666663\n",
      "0.4\n",
      "0.3846153846153846\n",
      "0.37037037037037035\n",
      "0.35714285714285715\n",
      "0.3448275862068965\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This learning rate decay scheme lowers the learning rate each step using the mentioned formula. Initially, the learning rate drops fast, but the change in the learning rate lowers each step, letting the model sit as close as possible to the minimum. The model needs small updates near the end of the training to be able to get as close to this point as possible. We can now update our SGD optimizer class to allow for the learning rate decay:",
   "id": "9f513eedd964a86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T02:26:30.189101Z",
     "start_time": "2025-04-07T02:26:30.176824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # initialize optimizer - set settings, learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iteration = 0\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iteration))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iteration += 1"
   ],
   "id": "3bce50600858762e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We've updated a few things in the SGD class. First, in the `__init__` method, we added handling for the current learning rate, and `self.learning_rate` is now the initial leaning rate. We also added attributes to track the decay rate and the number of iterations that the optimizer has gone through. Next, we added a new method called `pre_update_params`. This method, if we have a decay rate other than 0, will update our `self.current_learning_rate` using the prior formula. The `update_params` method remains unchanged, but we do have a new `post_update_params` method that will add to our `self.iterations` tracking. With out updated SGD optimizer class, we've added printing the current learning rate, and added pre and post optimizer method class. Let's use a decay rate of 1e-2 (0.01) and train our model again:",
   "id": "bebfb18b8731be1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T02:28:12.343186Z",
     "start_time": "2025-04-07T02:27:55.416249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay=1e-2)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, loss: {loss:.3f}, acc: {accuracy:.3f}, lr: {optimizer.current_learning_rate:}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n"
   ],
   "id": "ad7ee6d912a1bc3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.099, acc: 0.317, lr: 1.0\n",
      "epoch: 100, loss: 1.097, acc: 0.383, lr: 0.5025125628140703\n",
      "epoch: 200, loss: 1.091, acc: 0.380, lr: 0.33444816053511706\n",
      "epoch: 300, loss: 1.085, acc: 0.383, lr: 0.2506265664160401\n",
      "epoch: 400, loss: 1.083, acc: 0.380, lr: 0.2004008016032064\n",
      "epoch: 500, loss: 1.082, acc: 0.380, lr: 0.1669449081803005\n",
      "epoch: 600, loss: 1.081, acc: 0.383, lr: 0.14306151645207438\n",
      "epoch: 700, loss: 1.081, acc: 0.380, lr: 0.1251564455569462\n",
      "epoch: 800, loss: 1.081, acc: 0.380, lr: 0.11123470522803114\n",
      "epoch: 900, loss: 1.081, acc: 0.380, lr: 0.10010010010010009\n",
      "epoch: 1000, loss: 1.081, acc: 0.380, lr: 0.09099181073703366\n",
      "epoch: 1100, loss: 1.080, acc: 0.383, lr: 0.08340283569641367\n",
      "epoch: 1200, loss: 1.080, acc: 0.383, lr: 0.07698229407236336\n",
      "epoch: 1300, loss: 1.080, acc: 0.383, lr: 0.07147962830593281\n",
      "epoch: 1400, loss: 1.080, acc: 0.380, lr: 0.066711140760507\n",
      "epoch: 1500, loss: 1.080, acc: 0.380, lr: 0.06253908692933083\n",
      "epoch: 1600, loss: 1.080, acc: 0.380, lr: 0.05885815185403177\n",
      "epoch: 1700, loss: 1.080, acc: 0.380, lr: 0.055586436909394105\n",
      "epoch: 1800, loss: 1.080, acc: 0.380, lr: 0.052659294365455495\n",
      "epoch: 1900, loss: 1.080, acc: 0.377, lr: 0.05002501250625312\n",
      "epoch: 2000, loss: 1.080, acc: 0.373, lr: 0.047641734159123386\n",
      "epoch: 2100, loss: 1.080, acc: 0.377, lr: 0.04547521600727603\n",
      "epoch: 2200, loss: 1.080, acc: 0.380, lr: 0.04349717268377555\n",
      "epoch: 2300, loss: 1.080, acc: 0.373, lr: 0.04168403501458941\n",
      "epoch: 2400, loss: 1.080, acc: 0.377, lr: 0.04001600640256102\n",
      "epoch: 2500, loss: 1.080, acc: 0.377, lr: 0.03847633705271258\n",
      "epoch: 2600, loss: 1.080, acc: 0.377, lr: 0.03705075954057058\n",
      "epoch: 2700, loss: 1.080, acc: 0.377, lr: 0.03572704537334762\n",
      "epoch: 2800, loss: 1.080, acc: 0.377, lr: 0.03449465332873405\n",
      "epoch: 2900, loss: 1.080, acc: 0.377, lr: 0.03334444814938312\n",
      "epoch: 3000, loss: 1.080, acc: 0.377, lr: 0.03226847370119393\n",
      "epoch: 3100, loss: 1.080, acc: 0.377, lr: 0.03125976867771178\n",
      "epoch: 3200, loss: 1.080, acc: 0.377, lr: 0.03031221582297666\n",
      "epoch: 3300, loss: 1.080, acc: 0.377, lr: 0.02942041776993233\n",
      "epoch: 3400, loss: 1.080, acc: 0.377, lr: 0.028579594169762787\n",
      "epoch: 3500, loss: 1.080, acc: 0.377, lr: 0.027785495971103084\n",
      "epoch: 3600, loss: 1.080, acc: 0.373, lr: 0.02703433360367667\n",
      "epoch: 3700, loss: 1.080, acc: 0.373, lr: 0.026322716504343247\n",
      "epoch: 3800, loss: 1.080, acc: 0.373, lr: 0.025647601949217745\n",
      "epoch: 3900, loss: 1.080, acc: 0.373, lr: 0.02500625156289072\n",
      "epoch: 4000, loss: 1.080, acc: 0.377, lr: 0.02439619419370578\n",
      "epoch: 4100, loss: 1.080, acc: 0.380, lr: 0.023815194093831864\n",
      "epoch: 4200, loss: 1.080, acc: 0.380, lr: 0.02326122354035822\n",
      "epoch: 4300, loss: 1.080, acc: 0.380, lr: 0.022732439190725165\n",
      "epoch: 4400, loss: 1.080, acc: 0.380, lr: 0.02222716159146477\n",
      "epoch: 4500, loss: 1.080, acc: 0.380, lr: 0.021743857360295715\n",
      "epoch: 4600, loss: 1.080, acc: 0.380, lr: 0.021281123643328365\n",
      "epoch: 4700, loss: 1.080, acc: 0.380, lr: 0.02083767451552407\n",
      "epoch: 4800, loss: 1.080, acc: 0.380, lr: 0.020412329046744233\n",
      "epoch: 4900, loss: 1.079, acc: 0.380, lr: 0.020004000800160033\n",
      "epoch: 5000, loss: 1.079, acc: 0.380, lr: 0.019611688566385566\n",
      "epoch: 5100, loss: 1.079, acc: 0.380, lr: 0.019234468166955183\n",
      "epoch: 5200, loss: 1.079, acc: 0.380, lr: 0.018871485185884128\n",
      "epoch: 5300, loss: 1.079, acc: 0.380, lr: 0.018521948508983144\n",
      "epoch: 5400, loss: 1.079, acc: 0.380, lr: 0.01818512456810329\n",
      "epoch: 5500, loss: 1.079, acc: 0.380, lr: 0.01786033220217896\n",
      "epoch: 5600, loss: 1.079, acc: 0.380, lr: 0.01754693805930865\n",
      "epoch: 5700, loss: 1.079, acc: 0.380, lr: 0.01724435247456458\n",
      "epoch: 5800, loss: 1.079, acc: 0.380, lr: 0.016952025767079167\n",
      "epoch: 5900, loss: 1.079, acc: 0.380, lr: 0.01666944490748458\n",
      "epoch: 6000, loss: 1.079, acc: 0.383, lr: 0.016396130513198885\n",
      "epoch: 6100, loss: 1.079, acc: 0.383, lr: 0.016131634134537828\n",
      "epoch: 6200, loss: 1.079, acc: 0.383, lr: 0.015875535799333228\n",
      "epoch: 6300, loss: 1.079, acc: 0.383, lr: 0.01562744178777934\n",
      "epoch: 6400, loss: 1.079, acc: 0.383, lr: 0.015386982612709646\n",
      "epoch: 6500, loss: 1.079, acc: 0.383, lr: 0.015153811183512654\n",
      "epoch: 6600, loss: 1.079, acc: 0.383, lr: 0.014927601134497688\n",
      "epoch: 6700, loss: 1.079, acc: 0.383, lr: 0.014708045300779527\n",
      "epoch: 6800, loss: 1.079, acc: 0.383, lr: 0.014494854326714018\n",
      "epoch: 6900, loss: 1.079, acc: 0.383, lr: 0.014287755393627663\n",
      "epoch: 7000, loss: 1.079, acc: 0.383, lr: 0.014086491055078181\n",
      "epoch: 7100, loss: 1.079, acc: 0.383, lr: 0.013890818169190166\n",
      "epoch: 7200, loss: 1.079, acc: 0.383, lr: 0.013700506918755994\n",
      "epoch: 7300, loss: 1.079, acc: 0.383, lr: 0.013515339910798757\n",
      "epoch: 7400, loss: 1.079, acc: 0.383, lr: 0.013335111348179758\n",
      "epoch: 7500, loss: 1.079, acc: 0.383, lr: 0.013159626266614028\n",
      "epoch: 7600, loss: 1.079, acc: 0.383, lr: 0.012988699831146902\n",
      "epoch: 7700, loss: 1.079, acc: 0.383, lr: 0.012822156686754713\n",
      "epoch: 7800, loss: 1.079, acc: 0.383, lr: 0.0126598303582732\n",
      "epoch: 7900, loss: 1.079, acc: 0.383, lr: 0.012501562695336917\n",
      "epoch: 8000, loss: 1.079, acc: 0.383, lr: 0.012347203358439314\n",
      "epoch: 8100, loss: 1.079, acc: 0.387, lr: 0.012196609342602758\n",
      "epoch: 8200, loss: 1.079, acc: 0.387, lr: 0.012049644535486204\n",
      "epoch: 8300, loss: 1.079, acc: 0.387, lr: 0.011906179307060364\n",
      "epoch: 8400, loss: 1.079, acc: 0.390, lr: 0.011766090128250382\n",
      "epoch: 8500, loss: 1.079, acc: 0.390, lr: 0.01162925921618793\n",
      "epoch: 8600, loss: 1.079, acc: 0.390, lr: 0.011495574203931488\n",
      "epoch: 8700, loss: 1.079, acc: 0.390, lr: 0.011364927832708264\n",
      "epoch: 8800, loss: 1.079, acc: 0.390, lr: 0.011237217664906169\n",
      "epoch: 8900, loss: 1.079, acc: 0.390, lr: 0.011112345816201801\n",
      "epoch: 9000, loss: 1.079, acc: 0.390, lr: 0.010990218705352238\n",
      "epoch: 9100, loss: 1.079, acc: 0.390, lr: 0.010870746820306556\n",
      "epoch: 9200, loss: 1.079, acc: 0.390, lr: 0.010753844499408539\n",
      "epoch: 9300, loss: 1.079, acc: 0.390, lr: 0.010639429726566656\n",
      "epoch: 9400, loss: 1.079, acc: 0.390, lr: 0.010527423939362039\n",
      "epoch: 9500, loss: 1.079, acc: 0.390, lr: 0.010417751849150954\n",
      "epoch: 9600, loss: 1.079, acc: 0.390, lr: 0.010310341272296112\n",
      "epoch: 9700, loss: 1.079, acc: 0.390, lr: 0.010205122971731808\n",
      "epoch: 9800, loss: 1.079, acc: 0.390, lr: 0.010102030508132133\n",
      "epoch: 9900, loss: 1.079, acc: 0.390, lr: 0.01000100010001\n",
      "epoch: 10000, loss: 1.079, acc: 0.390, lr: 0.009901970492127933\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This model definitely got stuck, and the reason is almost certainly because the learning rate decayed far too quickly and became too small, trapping the model in some local minimum. This is most likely why, rather than wiggling, our accuracy and loss stopped chaning *at all*..\n",
    "\n",
    "We can, instead, try to decay a bit slower by making our decay a smaller number, say 1e-3:"
   ],
   "id": "a9d49f50b11b364c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T02:37:42.559392Z",
     "start_time": "2025-04-07T02:37:25.947533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ],
   "id": "b24c93cdf769ced4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.317, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.443, loss: 1.081, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.460, loss: 1.066, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.460, loss: 1.066, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.460, loss: 1.065, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.453, loss: 1.065, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.453, loss: 1.065, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.453, loss: 1.064, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.453, loss: 1.064, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.453, loss: 1.064, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.453, loss: 1.063, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.453, loss: 1.062, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.457, loss: 1.061, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.457, loss: 1.060, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.460, loss: 1.058, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.453, loss: 1.056, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.447, loss: 1.053, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.447, loss: 1.050, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.447, loss: 1.046, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.447, loss: 1.042, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.453, loss: 1.038, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.467, loss: 1.032, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.483, loss: 1.027, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.490, loss: 1.021, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.497, loss: 1.016, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.497, loss: 1.010, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.510, loss: 1.005, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.517, loss: 1.000, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.513, loss: 0.995, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.520, loss: 0.991, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.523, loss: 0.987, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.523, loss: 0.983, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.527, loss: 0.979, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.530, loss: 0.975, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.527, loss: 0.971, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.540, loss: 0.968, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.540, loss: 0.964, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.543, loss: 0.961, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.550, loss: 0.958, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.550, loss: 0.955, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.547, loss: 0.952, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.543, loss: 0.949, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.537, loss: 0.947, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.530, loss: 0.945, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.533, loss: 0.942, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.527, loss: 0.940, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.527, loss: 0.938, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.537, loss: 0.936, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.547, loss: 0.933, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.563, loss: 0.930, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.577, loss: 0.927, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.557, loss: 0.924, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.557, loss: 0.921, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.563, loss: 0.919, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.560, loss: 0.917, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.563, loss: 0.915, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.567, loss: 0.912, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.570, loss: 0.910, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.573, loss: 0.908, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.573, loss: 0.905, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.577, loss: 0.903, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.577, loss: 0.901, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.580, loss: 0.899, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.577, loss: 0.897, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.580, loss: 0.895, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.580, loss: 0.893, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.580, loss: 0.891, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.600, loss: 0.889, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.597, loss: 0.886, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.597, loss: 0.884, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.600, loss: 0.882, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.603, loss: 0.879, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.603, loss: 0.876, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.607, loss: 0.873, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.603, loss: 0.870, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.610, loss: 0.868, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.617, loss: 0.865, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.620, loss: 0.863, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.620, loss: 0.860, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.627, loss: 0.858, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.637, loss: 0.855, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.633, loss: 0.853, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.637, loss: 0.851, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.637, loss: 0.848, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.637, loss: 0.846, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.637, loss: 0.844, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.640, loss: 0.842, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.640, loss: 0.839, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.653, loss: 0.837, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.660, loss: 0.835, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.663, loss: 0.833, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.667, loss: 0.831, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.667, loss: 0.829, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.667, loss: 0.826, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.670, loss: 0.824, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.677, loss: 0.822, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.680, loss: 0.820, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.683, loss: 0.818, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.683, loss: 0.816, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.683, loss: 0.814, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.683, loss: 0.812, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Stochastic Gradient Descent with learning rate decay can do fairly well but is still a fairly basic optimization method that only follows a gradient without any additiional logic that could potentially help the model find the global minimum to the loss function. One option for improving the SGD optimizer is to introduce **momentum**.",
   "id": "99138fb7b9724026"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stochastic Gradient Descent with Momentum",
   "id": "9523178e0da3bf93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Momentum** creates a rolling average of gradients over some number of updates and uses this average with the unique gradient at each step. Another way of undestanding this is to imagine a ball going down a hill - even if it finds a small hole or hill, momentum will let it go straight through it towards a lower minimum - the bottom of this hill. This can help in cases where we're stuck in some local minimum (a hole), bouncing back and forth. With momentum, a model is more likely to pass through local minimum, further decreasing loss. Simply put, momentum may still point towards the global descent direction.\n",
    "\n",
    "With regular updates, the SGD optimizer might determine that the next best step is one that keeps the model in a local minimum. Remember that the gradient points toward the current steepest loss ascent for that step - taking the negative of the gradient vector flips it toward the current steepes descent, which may not necessarily follow descent toward the global minimum - the current steepest descent may point toward a local minimum. So this step may decrease loss for that update but might not get us out of the local minimum. We might wind up with a gradient that points in one direction and then the opposite direction in the next update; the gradient could continue to bounce back and forth around the local minimum like this, keeping the optimization of the loss stuck. Instead, momentum uses the previous update's direction to influence the next update's direction, minimizing the changes of bouncing around and getting stuck."
   ],
   "id": "70af3b65cea4a02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We utilize momentum by setting a parameter between 0 and 1, representing the fraction of the previous parameter update to retain, and subtracting (adding the negative) our actual gradient, multiplied by the learning rate (like before), from it. The update contains a portion of the gradient from preceding steps as our momentum (direction of previous changes) and only a portion of the current gradient; together, these portions form the actual change to our parameters and the bigger the role that momentum takes in the update, the slower the update can change the direction. When we set the momentum fraction too hight, the model might stop learning at all since the direction of the updates won't be able to follow the global gradient descent.\n",
    "\n",
    "<code>\n",
    "\n",
    "weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "\n",
    "</code>\n",
    "\n",
    "The hyperparameter, `self.momentum` is chosen at the start and the `layer.weight_momentums` starts as all zeros but are altered during training as:\n",
    "\n",
    "<code>\n",
    "\n",
    "layer.weight_momentums = weight_updates\n",
    "\n",
    "</code>\n",
    "\n",
    "\n",
    "\n",
    "This means that the momentum is always the previous update to the parameters. We will perform the same operations as the above with the biases. We can then update our SGD optimizer class' `update_params` method with the momentum calculation, applying with the parameters, and retaining them for the next steps as an alternative chain of operations to the current code. The difference is that we only calculate the updates and we add these updates with the common code:"
   ],
   "id": "bf8d043a367afe52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T03:08:32.201601Z",
     "start_time": "2025-04-07T03:08:32.188693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_params(self, layer):\n",
    "\n",
    "    # If we use momentum\n",
    "    if self.momentum:\n",
    "\n",
    "        # If layer does not contain momentum arrays, create them\n",
    "        # filled with zeros\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\n",
    "            # If there is no momentum array for weights\n",
    "            # The array doesn't exist for biases yet either\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Build weight updates with momentum - take previous updates\n",
    "        # multiplied by retained factor and update with current gradients\n",
    "        weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "\n",
    "        layer.weight_momentums = weight_updates\n",
    "\n",
    "        # build bias updates\n",
    "        bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "        layer.bias_momentums = bias_updates\n",
    "\n",
    "    # Vanilla SGD updates (as before momentum update)\n",
    "    else:\n",
    "        weight_updates = -self.current_learning_rate * layer.dweights\n",
    "        bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    # Update weights and biases using either vanilla or momentum updates\n",
    "    layer.weights += weight_updates\n",
    "    layer.biases += bias_updates\n"
   ],
   "id": "e5379461422d9858",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T03:39:18.879028Z",
     "start_time": "2025-04-07T03:39:18.867110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentum = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ],
   "id": "4ca1fa13b746c576",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T03:39:36.768Z",
     "start_time": "2025-04-07T03:39:19.655778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.5)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}. lr: {optimizer.current_learning_rate}, momentum: {optimizer.momentum}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ],
   "id": "10a983bfe4ebdb51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.363, loss: 1.099. lr: 1.0, momentum: 0.5\n",
      "epoch: 100, acc: 0.430, loss: 1.077. lr: 0.9099181073703367, momentum: 0.5\n",
      "epoch: 200, acc: 0.410, loss: 1.073. lr: 0.8340283569641367, momentum: 0.5\n",
      "epoch: 300, acc: 0.423, loss: 1.069. lr: 0.7698229407236336, momentum: 0.5\n",
      "epoch: 400, acc: 0.413, loss: 1.067. lr: 0.7147962830593281, momentum: 0.5\n",
      "epoch: 500, acc: 0.420, loss: 1.064. lr: 0.66711140760507, momentum: 0.5\n",
      "epoch: 600, acc: 0.433, loss: 1.058. lr: 0.6253908692933083, momentum: 0.5\n",
      "epoch: 700, acc: 0.420, loss: 1.048. lr: 0.5885815185403178, momentum: 0.5\n",
      "epoch: 800, acc: 0.470, loss: 1.035. lr: 0.5558643690939411, momentum: 0.5\n",
      "epoch: 900, acc: 0.497, loss: 1.023. lr: 0.526592943654555, momentum: 0.5\n",
      "epoch: 1000, acc: 0.477, loss: 1.009. lr: 0.5002501250625312, momentum: 0.5\n",
      "epoch: 1100, acc: 0.500, loss: 0.995. lr: 0.4764173415912339, momentum: 0.5\n",
      "epoch: 1200, acc: 0.520, loss: 0.979. lr: 0.45475216007276037, momentum: 0.5\n",
      "epoch: 1300, acc: 0.540, loss: 0.960. lr: 0.43497172683775553, momentum: 0.5\n",
      "epoch: 1400, acc: 0.507, loss: 0.964. lr: 0.4168403501458941, momentum: 0.5\n",
      "epoch: 1500, acc: 0.503, loss: 0.951. lr: 0.4001600640256102, momentum: 0.5\n",
      "epoch: 1600, acc: 0.510, loss: 0.940. lr: 0.3847633705271258, momentum: 0.5\n",
      "epoch: 1700, acc: 0.503, loss: 0.930. lr: 0.3705075954057058, momentum: 0.5\n",
      "epoch: 1800, acc: 0.527, loss: 0.916. lr: 0.35727045373347627, momentum: 0.5\n",
      "epoch: 1900, acc: 0.533, loss: 0.914. lr: 0.3449465332873405, momentum: 0.5\n",
      "epoch: 2000, acc: 0.563, loss: 0.936. lr: 0.33344448149383127, momentum: 0.5\n",
      "epoch: 2100, acc: 0.560, loss: 0.908. lr: 0.32268473701193934, momentum: 0.5\n",
      "epoch: 2200, acc: 0.610, loss: 0.886. lr: 0.31259768677711786, momentum: 0.5\n",
      "epoch: 2300, acc: 0.600, loss: 0.879. lr: 0.3031221582297666, momentum: 0.5\n",
      "epoch: 2400, acc: 0.547, loss: 0.901. lr: 0.29420417769932333, momentum: 0.5\n",
      "epoch: 2500, acc: 0.603, loss: 0.878. lr: 0.2857959416976279, momentum: 0.5\n",
      "epoch: 2600, acc: 0.550, loss: 0.869. lr: 0.2778549597110308, momentum: 0.5\n",
      "epoch: 2700, acc: 0.607, loss: 0.852. lr: 0.2703433360367667, momentum: 0.5\n",
      "epoch: 2800, acc: 0.627, loss: 0.844. lr: 0.26322716504343247, momentum: 0.5\n",
      "epoch: 2900, acc: 0.603, loss: 0.884. lr: 0.25647601949217746, momentum: 0.5\n",
      "epoch: 3000, acc: 0.620, loss: 0.833. lr: 0.25006251562890724, momentum: 0.5\n",
      "epoch: 3100, acc: 0.607, loss: 0.827. lr: 0.2439619419370578, momentum: 0.5\n",
      "epoch: 3200, acc: 0.617, loss: 0.830. lr: 0.23815194093831865, momentum: 0.5\n",
      "epoch: 3300, acc: 0.613, loss: 0.823. lr: 0.23261223540358225, momentum: 0.5\n",
      "epoch: 3400, acc: 0.643, loss: 0.812. lr: 0.22732439190725165, momentum: 0.5\n",
      "epoch: 3500, acc: 0.617, loss: 0.799. lr: 0.22227161591464767, momentum: 0.5\n",
      "epoch: 3600, acc: 0.640, loss: 0.787. lr: 0.21743857360295715, momentum: 0.5\n",
      "epoch: 3700, acc: 0.623, loss: 0.787. lr: 0.21281123643328367, momentum: 0.5\n",
      "epoch: 3800, acc: 0.640, loss: 0.776. lr: 0.20837674515524068, momentum: 0.5\n",
      "epoch: 3900, acc: 0.650, loss: 0.764. lr: 0.20412329046744235, momentum: 0.5\n",
      "epoch: 4000, acc: 0.660, loss: 0.768. lr: 0.2000400080016003, momentum: 0.5\n",
      "epoch: 4100, acc: 0.660, loss: 0.753. lr: 0.19611688566385566, momentum: 0.5\n",
      "epoch: 4200, acc: 0.630, loss: 0.773. lr: 0.19234468166955185, momentum: 0.5\n",
      "epoch: 4300, acc: 0.667, loss: 0.743. lr: 0.18871485185884126, momentum: 0.5\n",
      "epoch: 4400, acc: 0.677, loss: 0.746. lr: 0.18521948508983144, momentum: 0.5\n",
      "epoch: 4500, acc: 0.647, loss: 0.745. lr: 0.18185124568103292, momentum: 0.5\n",
      "epoch: 4600, acc: 0.667, loss: 0.727. lr: 0.1786033220217896, momentum: 0.5\n",
      "epoch: 4700, acc: 0.673, loss: 0.723. lr: 0.1754693805930865, momentum: 0.5\n",
      "epoch: 4800, acc: 0.687, loss: 0.721. lr: 0.17244352474564578, momentum: 0.5\n",
      "epoch: 4900, acc: 0.670, loss: 0.710. lr: 0.16952025767079165, momentum: 0.5\n",
      "epoch: 5000, acc: 0.680, loss: 0.712. lr: 0.16669444907484582, momentum: 0.5\n",
      "epoch: 5100, acc: 0.683, loss: 0.699. lr: 0.16396130513198884, momentum: 0.5\n",
      "epoch: 5200, acc: 0.700, loss: 0.694. lr: 0.16131634134537828, momentum: 0.5\n",
      "epoch: 5300, acc: 0.687, loss: 0.686. lr: 0.15875535799333226, momentum: 0.5\n",
      "epoch: 5400, acc: 0.703, loss: 0.699. lr: 0.1562744178777934, momentum: 0.5\n",
      "epoch: 5500, acc: 0.737, loss: 0.672. lr: 0.15386982612709646, momentum: 0.5\n",
      "epoch: 5600, acc: 0.697, loss: 0.667. lr: 0.15153811183512653, momentum: 0.5\n",
      "epoch: 5700, acc: 0.717, loss: 0.669. lr: 0.14927601134497687, momentum: 0.5\n",
      "epoch: 5800, acc: 0.707, loss: 0.677. lr: 0.14708045300779526, momentum: 0.5\n",
      "epoch: 5900, acc: 0.743, loss: 0.655. lr: 0.14494854326714016, momentum: 0.5\n",
      "epoch: 6000, acc: 0.720, loss: 0.654. lr: 0.1428775539362766, momentum: 0.5\n",
      "epoch: 6100, acc: 0.710, loss: 0.670. lr: 0.1408649105507818, momentum: 0.5\n",
      "epoch: 6200, acc: 0.713, loss: 0.653. lr: 0.13890818169190167, momentum: 0.5\n",
      "epoch: 6300, acc: 0.720, loss: 0.638. lr: 0.13700506918755992, momentum: 0.5\n",
      "epoch: 6400, acc: 0.713, loss: 0.652. lr: 0.13515339910798757, momentum: 0.5\n",
      "epoch: 6500, acc: 0.717, loss: 0.639. lr: 0.13335111348179757, momentum: 0.5\n",
      "epoch: 6600, acc: 0.723, loss: 0.630. lr: 0.13159626266614027, momentum: 0.5\n",
      "epoch: 6700, acc: 0.717, loss: 0.645. lr: 0.12988699831146902, momentum: 0.5\n",
      "epoch: 6800, acc: 0.713, loss: 0.638. lr: 0.12822156686754713, momentum: 0.5\n",
      "epoch: 6900, acc: 0.727, loss: 0.628. lr: 0.126598303582732, momentum: 0.5\n",
      "epoch: 7000, acc: 0.723, loss: 0.622. lr: 0.12501562695336915, momentum: 0.5\n",
      "epoch: 7100, acc: 0.723, loss: 0.625. lr: 0.12347203358439313, momentum: 0.5\n",
      "epoch: 7200, acc: 0.730, loss: 0.622. lr: 0.12196609342602757, momentum: 0.5\n",
      "epoch: 7300, acc: 0.723, loss: 0.607. lr: 0.12049644535486204, momentum: 0.5\n",
      "epoch: 7400, acc: 0.730, loss: 0.612. lr: 0.11906179307060363, momentum: 0.5\n",
      "epoch: 7500, acc: 0.717, loss: 0.609. lr: 0.11766090128250381, momentum: 0.5\n",
      "epoch: 7600, acc: 0.730, loss: 0.595. lr: 0.11629259216187929, momentum: 0.5\n",
      "epoch: 7700, acc: 0.743, loss: 0.602. lr: 0.11495574203931487, momentum: 0.5\n",
      "epoch: 7800, acc: 0.723, loss: 0.594. lr: 0.11364927832708263, momentum: 0.5\n",
      "epoch: 7900, acc: 0.730, loss: 0.586. lr: 0.11237217664906168, momentum: 0.5\n",
      "epoch: 8000, acc: 0.747, loss: 0.586. lr: 0.11112345816201799, momentum: 0.5\n",
      "epoch: 8100, acc: 0.730, loss: 0.582. lr: 0.10990218705352237, momentum: 0.5\n",
      "epoch: 8200, acc: 0.740, loss: 0.574. lr: 0.10870746820306555, momentum: 0.5\n",
      "epoch: 8300, acc: 0.743, loss: 0.572. lr: 0.1075384449940854, momentum: 0.5\n",
      "epoch: 8400, acc: 0.747, loss: 0.570. lr: 0.10639429726566654, momentum: 0.5\n",
      "epoch: 8500, acc: 0.743, loss: 0.566. lr: 0.10527423939362038, momentum: 0.5\n",
      "epoch: 8600, acc: 0.750, loss: 0.563. lr: 0.10417751849150952, momentum: 0.5\n",
      "epoch: 8700, acc: 0.747, loss: 0.561. lr: 0.10310341272296113, momentum: 0.5\n",
      "epoch: 8800, acc: 0.757, loss: 0.558. lr: 0.1020512297173181, momentum: 0.5\n",
      "epoch: 8900, acc: 0.757, loss: 0.555. lr: 0.10102030508132134, momentum: 0.5\n",
      "epoch: 9000, acc: 0.757, loss: 0.553. lr: 0.1000100010001, momentum: 0.5\n",
      "epoch: 9100, acc: 0.757, loss: 0.551. lr: 0.09901970492127933, momentum: 0.5\n",
      "epoch: 9200, acc: 0.757, loss: 0.549. lr: 0.09804882831650162, momentum: 0.5\n",
      "epoch: 9300, acc: 0.757, loss: 0.547. lr: 0.09709680551509856, momentum: 0.5\n",
      "epoch: 9400, acc: 0.763, loss: 0.544. lr: 0.09616309260505818, momentum: 0.5\n",
      "epoch: 9500, acc: 0.763, loss: 0.542. lr: 0.09524716639679968, momentum: 0.5\n",
      "epoch: 9600, acc: 0.763, loss: 0.541. lr: 0.09434852344560807, momentum: 0.5\n",
      "epoch: 9700, acc: 0.767, loss: 0.538. lr: 0.09346667912889055, momentum: 0.5\n",
      "epoch: 9800, acc: 0.770, loss: 0.536. lr: 0.09260116677470137, momentum: 0.5\n",
      "epoch: 9900, acc: 0.777, loss: 0.534. lr: 0.09175153683824203, momentum: 0.5\n",
      "epoch: 10000, acc: 0.777, loss: 0.532. lr: 0.09091735612328393, momentum: 0.5\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:30:33.952701Z",
     "start_time": "2025-04-08T05:30:16.233428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try momentum is 0.9\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}\")\n",
    "\n",
    "    # Bacward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ],
   "id": "5f4bb455c4157739",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.323, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.503, loss: 1.039, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.497, loss: 0.982, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.597, loss: 0.951, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.537, loss: 1.004, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.560, loss: 0.896, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.603, loss: 0.844, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.667, loss: 0.803, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.637, loss: 0.880, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.557, loss: 0.909, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.540, loss: 0.938, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.573, loss: 0.944, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.570, loss: 0.852, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.587, loss: 0.866, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.570, loss: 0.949, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.693, loss: 0.750, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.663, loss: 0.804, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.580, loss: 0.945, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.690, loss: 0.699, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.640, loss: 0.848, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.757, loss: 0.631, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.753, loss: 0.653, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.697, loss: 0.735, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.750, loss: 0.632, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.667, loss: 0.762, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.693, loss: 0.729, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.733, loss: 0.638, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.727, loss: 0.639, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.760, loss: 0.582, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.713, loss: 0.686, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.767, loss: 0.566, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.753, loss: 0.563, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.790, loss: 0.506, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.733, loss: 0.666, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.793, loss: 0.510, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.800, loss: 0.511, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.823, loss: 0.467, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.827, loss: 0.456, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.830, loss: 0.447, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.833, loss: 0.447, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.753, loss: 0.559, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.840, loss: 0.429, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.830, loss: 0.437, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.800, loss: 0.503, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.860, loss: 0.392, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.860, loss: 0.384, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.713, loss: 0.742, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.840, loss: 0.400, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.863, loss: 0.362, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.863, loss: 0.363, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.900, loss: 0.353, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.900, loss: 0.346, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.860, loss: 0.360, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.877, loss: 0.341, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.867, loss: 0.335, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.887, loss: 0.332, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.877, loss: 0.319, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.857, loss: 0.364, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.807, loss: 0.481, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.907, loss: 0.309, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.907, loss: 0.303, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.907, loss: 0.307, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.860, loss: 0.364, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.897, loss: 0.300, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.903, loss: 0.297, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.903, loss: 0.292, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.897, loss: 0.293, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.897, loss: 0.293, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.903, loss: 0.282, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.903, loss: 0.280, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.910, loss: 0.274, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.907, loss: 0.270, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.910, loss: 0.267, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.913, loss: 0.263, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.913, loss: 0.261, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.913, loss: 0.258, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.920, loss: 0.255, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.920, loss: 0.252, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.920, loss: 0.251, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.917, loss: 0.249, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.917, loss: 0.247, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.923, loss: 0.244, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.923, loss: 0.242, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.923, loss: 0.240, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.927, loss: 0.238, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.930, loss: 0.236, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.930, loss: 0.235, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.930, loss: 0.233, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.930, loss: 0.231, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.930, loss: 0.230, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.930, loss: 0.228, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.930, loss: 0.226, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.933, loss: 0.224, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.933, loss: 0.223, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.937, loss: 0.222, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.937, loss: 0.221, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.937, loss: 0.219, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.943, loss: 0.217, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.947, loss: 0.216, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.943, loss: 0.215, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.947, loss: 0.214, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AdaGrad",
   "id": "283181bab26d9df1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Short **adaptive gradient**, institutes a per-parameter learning rate rather than a globally-shared rate. The idea here is to normalize updates made to the features. During the training process, some weights can rise significantly, while others tend to not change by much. It is usually better for weights to not rise too high compared to the other weights, which will be discusses with regularization techniques. Adagrad provides a way to normalize parameter updates by keepong a history of previous updates - the bigger the sum of the updates is, in either direction (positive or negative), the smaller updates are made further in training. This leets less-frequently updated parameters to keep up with changes, effectively utilizing more neurons for training. The concept of AdaGrad can be contained in the following two lines of code:\n",
    "\n",
    "<code>\n",
    "\n",
    "cache += param_gradient ** 2\n",
    "param_updates = learning_rate * param_gradient / (sqrt(cache) + eps)\n",
    "\n",
    "</code>\n",
    "\n",
    "The `cache` holds a history of squared gradients, and the `param_updates` is a function of the learning rate multiplied by the gradient (basic SGD so far) and then is divided by the square root of the cache plus some **epsilon** value. The division operation performed with a constantly rising cache might also cause the learning to tall as updates become smaller with time, due to the monotimic nature of updates. That's why this optimizer is not widely used, except for some specific application. The epsilon is hyperparameter (pre-training control knob setting) preventing division by 0. We also sum the squared values and taking the square root.\n",
    "\n",
    "The resulting cache value grows slower, and in a different way, taking care of the negative numbers (we would not want to divide the update by the negative number and flip its sign). Overall, the impact is the learning rates for parameters with smaller gradients are decreased slowly, while the parameters with larger gradients have their learning rates decreased faster."
   ],
   "id": "8a56f8d9bad3602c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:53:16.115734Z",
     "start_time": "2025-04-08T05:53:16.104866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# AdaGrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # call once befero any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with squared rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any paramter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "b9e6a66f450b2ada",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Testing this optimizer with decaying set to *1e-4* as well as *1e-5* works better than *1e-3*. This optimizer with our dataset works better with lesser decaying:",
   "id": "6c80bf591ee89527"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T05:58:07.975825Z",
     "start_time": "2025-04-08T05:57:50.221118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adagrad(decay=1e-4)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ],
   "id": "f48dfe015e48159a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.327, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.497, loss: 0.966, lr: 0.9901970492127933\n",
      "epoch: 200, acc: 0.537, loss: 0.922, lr: 0.9804882831650161\n",
      "epoch: 300, acc: 0.563, loss: 0.880, lr: 0.9709680551509855\n",
      "epoch: 400, acc: 0.607, loss: 0.845, lr: 0.9616309260505818\n",
      "epoch: 500, acc: 0.640, loss: 0.808, lr: 0.9524716639679969\n",
      "epoch: 600, acc: 0.603, loss: 0.777, lr: 0.9434852344560807\n",
      "epoch: 700, acc: 0.660, loss: 0.780, lr: 0.9346667912889054\n",
      "epoch: 800, acc: 0.677, loss: 0.711, lr: 0.9260116677470135\n",
      "epoch: 900, acc: 0.663, loss: 0.679, lr: 0.9175153683824203\n",
      "epoch: 1000, acc: 0.683, loss: 0.648, lr: 0.9091735612328392\n",
      "epoch: 1100, acc: 0.687, loss: 0.628, lr: 0.9009820704567978\n",
      "epoch: 1200, acc: 0.720, loss: 0.608, lr: 0.892936869363336\n",
      "epoch: 1300, acc: 0.723, loss: 0.595, lr: 0.8850340738118416\n",
      "epoch: 1400, acc: 0.757, loss: 0.572, lr: 0.8772699359592947\n",
      "epoch: 1500, acc: 0.770, loss: 0.562, lr: 0.8696408383337683\n",
      "epoch: 1600, acc: 0.790, loss: 0.551, lr: 0.8621432882145013\n",
      "epoch: 1700, acc: 0.783, loss: 0.549, lr: 0.8547739123001966\n",
      "epoch: 1800, acc: 0.797, loss: 0.530, lr: 0.8475294516484448\n",
      "epoch: 1900, acc: 0.803, loss: 0.524, lr: 0.8404067568703253\n",
      "epoch: 2000, acc: 0.800, loss: 0.515, lr: 0.8334027835652972\n",
      "epoch: 2100, acc: 0.787, loss: 0.514, lr: 0.8265145879824779\n",
      "epoch: 2200, acc: 0.800, loss: 0.503, lr: 0.8197393228953193\n",
      "epoch: 2300, acc: 0.797, loss: 0.497, lr: 0.8130742336775347\n",
      "epoch: 2400, acc: 0.803, loss: 0.489, lr: 0.8065166545689169\n",
      "epoch: 2500, acc: 0.807, loss: 0.483, lr: 0.8000640051204096\n",
      "epoch: 2600, acc: 0.807, loss: 0.478, lr: 0.7937137868084768\n",
      "epoch: 2700, acc: 0.813, loss: 0.468, lr: 0.7874635798094338\n",
      "epoch: 2800, acc: 0.807, loss: 0.466, lr: 0.7813110399249941\n",
      "epoch: 2900, acc: 0.807, loss: 0.458, lr: 0.7752538956508256\n",
      "epoch: 3000, acc: 0.813, loss: 0.451, lr: 0.7692899453804138\n",
      "epoch: 3100, acc: 0.820, loss: 0.447, lr: 0.7634170547370028\n",
      "epoch: 3200, acc: 0.820, loss: 0.442, lr: 0.7576331540268202\n",
      "epoch: 3300, acc: 0.827, loss: 0.441, lr: 0.7519362358072035\n",
      "epoch: 3400, acc: 0.823, loss: 0.435, lr: 0.7463243525636241\n",
      "epoch: 3500, acc: 0.820, loss: 0.431, lr: 0.7407956144899621\n",
      "epoch: 3600, acc: 0.827, loss: 0.427, lr: 0.735348187366718\n",
      "epoch: 3700, acc: 0.827, loss: 0.423, lr: 0.7299802905321557\n",
      "epoch: 3800, acc: 0.830, loss: 0.419, lr: 0.7246901949416624\n",
      "epoch: 3900, acc: 0.840, loss: 0.416, lr: 0.7194762213108857\n",
      "epoch: 4000, acc: 0.833, loss: 0.411, lr: 0.7143367383384527\n",
      "epoch: 4100, acc: 0.840, loss: 0.408, lr: 0.7092701610043266\n",
      "epoch: 4200, acc: 0.837, loss: 0.405, lr: 0.7042749489400663\n",
      "epoch: 4300, acc: 0.837, loss: 0.402, lr: 0.6993496048674733\n",
      "epoch: 4400, acc: 0.837, loss: 0.398, lr: 0.6944926731022988\n",
      "epoch: 4500, acc: 0.837, loss: 0.395, lr: 0.6897027381198704\n",
      "epoch: 4600, acc: 0.847, loss: 0.393, lr: 0.6849784231796698\n",
      "epoch: 4700, acc: 0.843, loss: 0.389, lr: 0.6803183890060548\n",
      "epoch: 4800, acc: 0.850, loss: 0.387, lr: 0.6757213325224677\n",
      "epoch: 4900, acc: 0.843, loss: 0.383, lr: 0.6711859856366199\n",
      "epoch: 5000, acc: 0.843, loss: 0.380, lr: 0.6667111140742716\n",
      "epoch: 5100, acc: 0.850, loss: 0.378, lr: 0.6622955162593549\n",
      "epoch: 5200, acc: 0.850, loss: 0.375, lr: 0.6579380222383051\n",
      "epoch: 5300, acc: 0.853, loss: 0.373, lr: 0.6536374926465782\n",
      "epoch: 5400, acc: 0.853, loss: 0.371, lr: 0.649392817715436\n",
      "epoch: 5500, acc: 0.860, loss: 0.368, lr: 0.6452029163171817\n",
      "epoch: 5600, acc: 0.860, loss: 0.366, lr: 0.6410667350471184\n",
      "epoch: 5700, acc: 0.860, loss: 0.364, lr: 0.6369832473405949\n",
      "epoch: 5800, acc: 0.860, loss: 0.362, lr: 0.6329514526235838\n",
      "epoch: 5900, acc: 0.860, loss: 0.360, lr: 0.6289703754953141\n",
      "epoch: 6000, acc: 0.860, loss: 0.358, lr: 0.6250390649415589\n",
      "epoch: 6100, acc: 0.860, loss: 0.356, lr: 0.6211565935772407\n",
      "epoch: 6200, acc: 0.860, loss: 0.352, lr: 0.6173220569170937\n",
      "epoch: 6300, acc: 0.860, loss: 0.351, lr: 0.6135345726731701\n",
      "epoch: 6400, acc: 0.857, loss: 0.349, lr: 0.6097932800780536\n",
      "epoch: 6500, acc: 0.857, loss: 0.348, lr: 0.6060973392326807\n",
      "epoch: 6600, acc: 0.857, loss: 0.346, lr: 0.6024459304777396\n",
      "epoch: 6700, acc: 0.857, loss: 0.345, lr: 0.5988382537876519\n",
      "epoch: 6800, acc: 0.857, loss: 0.343, lr: 0.5952735281862016\n",
      "epoch: 6900, acc: 0.857, loss: 0.342, lr: 0.5917509911829102\n",
      "epoch: 7000, acc: 0.857, loss: 0.341, lr: 0.5882698982293076\n",
      "epoch: 7100, acc: 0.857, loss: 0.339, lr: 0.5848295221942803\n",
      "epoch: 7200, acc: 0.857, loss: 0.337, lr: 0.5814291528577243\n",
      "epoch: 7300, acc: 0.863, loss: 0.336, lr: 0.5780680964217585\n",
      "epoch: 7400, acc: 0.863, loss: 0.334, lr: 0.5747456750387954\n",
      "epoch: 7500, acc: 0.863, loss: 0.333, lr: 0.5714612263557918\n",
      "epoch: 7600, acc: 0.863, loss: 0.332, lr: 0.5682141030740383\n",
      "epoch: 7700, acc: 0.863, loss: 0.330, lr: 0.5650036725238714\n",
      "epoch: 7800, acc: 0.863, loss: 0.329, lr: 0.5618293162537221\n",
      "epoch: 7900, acc: 0.863, loss: 0.328, lr: 0.5586904296329404\n",
      "epoch: 8000, acc: 0.873, loss: 0.326, lr: 0.5555864214678593\n",
      "epoch: 8100, acc: 0.873, loss: 0.317, lr: 0.5525167136305873\n",
      "epoch: 8200, acc: 0.890, loss: 0.312, lr: 0.5494807407000385\n",
      "epoch: 8300, acc: 0.897, loss: 0.307, lr: 0.5464779496147331\n",
      "epoch: 8400, acc: 0.897, loss: 0.305, lr: 0.5435077993369205\n",
      "epoch: 8500, acc: 0.897, loss: 0.302, lr: 0.5405697605275961\n",
      "epoch: 8600, acc: 0.897, loss: 0.301, lr: 0.5376633152320017\n",
      "epoch: 8700, acc: 0.900, loss: 0.299, lr: 0.5347879565752179\n",
      "epoch: 8800, acc: 0.900, loss: 0.297, lr: 0.5319431884674717\n",
      "epoch: 8900, acc: 0.900, loss: 0.296, lr: 0.5291285253188\n",
      "epoch: 9000, acc: 0.900, loss: 0.294, lr: 0.5263434917627243\n",
      "epoch: 9100, acc: 0.900, loss: 0.293, lr: 0.5235876223886068\n",
      "epoch: 9200, acc: 0.900, loss: 0.291, lr: 0.5208604614823689\n",
      "epoch: 9300, acc: 0.900, loss: 0.290, lr: 0.5181615627752734\n",
      "epoch: 9400, acc: 0.900, loss: 0.289, lr: 0.5154904892004742\n",
      "epoch: 9500, acc: 0.900, loss: 0.287, lr: 0.5128468126570593\n",
      "epoch: 9600, acc: 0.903, loss: 0.286, lr: 0.5102301137813153\n",
      "epoch: 9700, acc: 0.903, loss: 0.285, lr: 0.5076399817249606\n",
      "epoch: 9800, acc: 0.903, loss: 0.284, lr: 0.5050760139400979\n",
      "epoch: 9900, acc: 0.903, loss: 0.283, lr: 0.5025378159706518\n",
      "epoch: 10000, acc: 0.903, loss: 0.282, lr: 0.5000250012500626\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "AdaGrad worked quite well here, but not as good as SGD with momentum, and we can see that loss consistently fell through the enture training process.",
   "id": "689bdeaf207fd6bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RMSProp",
   "id": "50e6b8eaa80d4521"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Short for **Root Mean Square Propagation**. Similar to AdaGrad, RMSProp calculates an adaptive learning rate per parameter, it's just calculated in a different way than AdaGrad.\n",
    "\n",
    "<code>\n",
    "\n",
    "cache = rho * cache + (1 - rho) * gradient ** 2\n",
    "\n",
    "</code>\n",
    "\n",
    "This is similar to both momentum with the SGD optimizer and cache with the AdaGrad. RMSProp adds a mechanism similar to momentum but also adds a per-parameter adaptive learning rate, so the learning rate changes are smoother. This helps to retain the global direction of changes and slows changes in direction. Instead of continually adding squred gradients to a cache (like in AdaGrad), it uses a moving average of the cache. Each update to the cache retains a part of the cache and updates it with a fraction of the new, squared gradients. In this way, cache contents \"move\" with data in time, and learning does not stall. In the case of this optimizer, the per-parameter learning rate can either fall or rise, depending on the last updates and current gradient. RMSProp applies the cache in the same way as AdaGrad does.\n",
    "\n",
    "The new hyperparameter here is *rho*. *rho* is the cache memory decay rate. Because this optimizer, with default values, carries over so much momentum of gradient and the adaptive learning rate updates, even small gradient updates are enough to keep it going; therefore, a default learning rate of *1* is far too large and causes instant model instability. A learning rate that becomes stable again and gives fast enough updates is around *0.001* (that's also the default value for this optimizer used in well-known machine learning frameworks). That's waht we'll use as default from now on too."
   ],
   "id": "35d64bd6514920bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T06:07:48.656570Z",
     "start_time": "2025-04-09T06:07:48.644706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RMSProp optimizer\n",
    "class Optimizer_RMSProp:\n",
    "\n",
    "    # initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradient\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "10686cad616430d8",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T06:11:07.193863Z",
     "start_time": "2025-04-09T06:10:49.031116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_RMSProp(decay=1e-4)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}. loss: {loss:.3f}, lr: {optimizer.current_learning_rate:.9f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ],
   "id": "97d37e0cd834200c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360. loss: 1.099, lr: 0.001000000\n",
      "epoch: 100, acc: 0.403. loss: 1.071, lr: 0.000990197\n",
      "epoch: 200, acc: 0.410. loss: 1.063, lr: 0.000980488\n",
      "epoch: 300, acc: 0.437. loss: 1.058, lr: 0.000970968\n",
      "epoch: 400, acc: 0.483. loss: 1.054, lr: 0.000961631\n",
      "epoch: 500, acc: 0.500. loss: 1.049, lr: 0.000952472\n",
      "epoch: 600, acc: 0.493. loss: 1.043, lr: 0.000943485\n",
      "epoch: 700, acc: 0.487. loss: 1.036, lr: 0.000934667\n",
      "epoch: 800, acc: 0.483. loss: 1.027, lr: 0.000926012\n",
      "epoch: 900, acc: 0.493. loss: 1.018, lr: 0.000917515\n",
      "epoch: 1000, acc: 0.507. loss: 1.009, lr: 0.000909174\n",
      "epoch: 1100, acc: 0.513. loss: 1.001, lr: 0.000900982\n",
      "epoch: 1200, acc: 0.523. loss: 0.992, lr: 0.000892937\n",
      "epoch: 1300, acc: 0.537. loss: 0.984, lr: 0.000885034\n",
      "epoch: 1400, acc: 0.513. loss: 0.977, lr: 0.000877270\n",
      "epoch: 1500, acc: 0.520. loss: 0.970, lr: 0.000869641\n",
      "epoch: 1600, acc: 0.537. loss: 0.963, lr: 0.000862143\n",
      "epoch: 1700, acc: 0.557. loss: 0.957, lr: 0.000854774\n",
      "epoch: 1800, acc: 0.557. loss: 0.950, lr: 0.000847529\n",
      "epoch: 1900, acc: 0.567. loss: 0.943, lr: 0.000840407\n",
      "epoch: 2000, acc: 0.570. loss: 0.937, lr: 0.000833403\n",
      "epoch: 2100, acc: 0.577. loss: 0.931, lr: 0.000826515\n",
      "epoch: 2200, acc: 0.583. loss: 0.924, lr: 0.000819739\n",
      "epoch: 2300, acc: 0.587. loss: 0.917, lr: 0.000813074\n",
      "epoch: 2400, acc: 0.590. loss: 0.911, lr: 0.000806517\n",
      "epoch: 2500, acc: 0.587. loss: 0.904, lr: 0.000800064\n",
      "epoch: 2600, acc: 0.587. loss: 0.898, lr: 0.000793714\n",
      "epoch: 2700, acc: 0.583. loss: 0.893, lr: 0.000787464\n",
      "epoch: 2800, acc: 0.583. loss: 0.888, lr: 0.000781311\n",
      "epoch: 2900, acc: 0.610. loss: 0.883, lr: 0.000775254\n",
      "epoch: 3000, acc: 0.590. loss: 0.878, lr: 0.000769290\n",
      "epoch: 3100, acc: 0.600. loss: 0.874, lr: 0.000763417\n",
      "epoch: 3200, acc: 0.610. loss: 0.870, lr: 0.000757633\n",
      "epoch: 3300, acc: 0.597. loss: 0.866, lr: 0.000751936\n",
      "epoch: 3400, acc: 0.623. loss: 0.862, lr: 0.000746324\n",
      "epoch: 3500, acc: 0.603. loss: 0.858, lr: 0.000740796\n",
      "epoch: 3600, acc: 0.627. loss: 0.855, lr: 0.000735348\n",
      "epoch: 3700, acc: 0.610. loss: 0.851, lr: 0.000729980\n",
      "epoch: 3800, acc: 0.613. loss: 0.848, lr: 0.000724690\n",
      "epoch: 3900, acc: 0.610. loss: 0.845, lr: 0.000719476\n",
      "epoch: 4000, acc: 0.633. loss: 0.842, lr: 0.000714337\n",
      "epoch: 4100, acc: 0.627. loss: 0.839, lr: 0.000709270\n",
      "epoch: 4200, acc: 0.630. loss: 0.837, lr: 0.000704275\n",
      "epoch: 4300, acc: 0.617. loss: 0.834, lr: 0.000699350\n",
      "epoch: 4400, acc: 0.617. loss: 0.832, lr: 0.000694493\n",
      "epoch: 4500, acc: 0.627. loss: 0.830, lr: 0.000689703\n",
      "epoch: 4600, acc: 0.630. loss: 0.827, lr: 0.000684978\n",
      "epoch: 4700, acc: 0.613. loss: 0.825, lr: 0.000680318\n",
      "epoch: 4800, acc: 0.613. loss: 0.823, lr: 0.000675721\n",
      "epoch: 4900, acc: 0.633. loss: 0.821, lr: 0.000671186\n",
      "epoch: 5000, acc: 0.630. loss: 0.819, lr: 0.000666711\n",
      "epoch: 5100, acc: 0.630. loss: 0.817, lr: 0.000662296\n",
      "epoch: 5200, acc: 0.623. loss: 0.815, lr: 0.000657938\n",
      "epoch: 5300, acc: 0.643. loss: 0.813, lr: 0.000653637\n",
      "epoch: 5400, acc: 0.643. loss: 0.812, lr: 0.000649393\n",
      "epoch: 5500, acc: 0.623. loss: 0.810, lr: 0.000645203\n",
      "epoch: 5600, acc: 0.630. loss: 0.807, lr: 0.000641067\n",
      "epoch: 5700, acc: 0.670. loss: 0.804, lr: 0.000636983\n",
      "epoch: 5800, acc: 0.677. loss: 0.800, lr: 0.000632951\n",
      "epoch: 5900, acc: 0.643. loss: 0.796, lr: 0.000628970\n",
      "epoch: 6000, acc: 0.650. loss: 0.793, lr: 0.000625039\n",
      "epoch: 6100, acc: 0.667. loss: 0.791, lr: 0.000621157\n",
      "epoch: 6200, acc: 0.670. loss: 0.789, lr: 0.000617322\n",
      "epoch: 6300, acc: 0.650. loss: 0.787, lr: 0.000613535\n",
      "epoch: 6400, acc: 0.663. loss: 0.786, lr: 0.000609793\n",
      "epoch: 6500, acc: 0.660. loss: 0.784, lr: 0.000606097\n",
      "epoch: 6600, acc: 0.650. loss: 0.782, lr: 0.000602446\n",
      "epoch: 6700, acc: 0.653. loss: 0.781, lr: 0.000598838\n",
      "epoch: 6800, acc: 0.663. loss: 0.779, lr: 0.000595274\n",
      "epoch: 6900, acc: 0.667. loss: 0.777, lr: 0.000591751\n",
      "epoch: 7000, acc: 0.660. loss: 0.776, lr: 0.000588270\n",
      "epoch: 7100, acc: 0.660. loss: 0.775, lr: 0.000584830\n",
      "epoch: 7200, acc: 0.670. loss: 0.773, lr: 0.000581429\n",
      "epoch: 7300, acc: 0.670. loss: 0.772, lr: 0.000578068\n",
      "epoch: 7400, acc: 0.667. loss: 0.770, lr: 0.000574746\n",
      "epoch: 7500, acc: 0.667. loss: 0.769, lr: 0.000571461\n",
      "epoch: 7600, acc: 0.667. loss: 0.768, lr: 0.000568214\n",
      "epoch: 7700, acc: 0.660. loss: 0.766, lr: 0.000565004\n",
      "epoch: 7800, acc: 0.657. loss: 0.765, lr: 0.000561829\n",
      "epoch: 7900, acc: 0.663. loss: 0.764, lr: 0.000558690\n",
      "epoch: 8000, acc: 0.667. loss: 0.763, lr: 0.000555586\n",
      "epoch: 8100, acc: 0.663. loss: 0.761, lr: 0.000552517\n",
      "epoch: 8200, acc: 0.667. loss: 0.760, lr: 0.000549481\n",
      "epoch: 8300, acc: 0.667. loss: 0.759, lr: 0.000546478\n",
      "epoch: 8400, acc: 0.660. loss: 0.758, lr: 0.000543508\n",
      "epoch: 8500, acc: 0.657. loss: 0.757, lr: 0.000540570\n",
      "epoch: 8600, acc: 0.660. loss: 0.756, lr: 0.000537663\n",
      "epoch: 8700, acc: 0.667. loss: 0.755, lr: 0.000534788\n",
      "epoch: 8800, acc: 0.663. loss: 0.754, lr: 0.000531943\n",
      "epoch: 8900, acc: 0.670. loss: 0.753, lr: 0.000529129\n",
      "epoch: 9000, acc: 0.670. loss: 0.752, lr: 0.000526343\n",
      "epoch: 9100, acc: 0.663. loss: 0.751, lr: 0.000523588\n",
      "epoch: 9200, acc: 0.667. loss: 0.750, lr: 0.000520860\n",
      "epoch: 9300, acc: 0.667. loss: 0.749, lr: 0.000518162\n",
      "epoch: 9400, acc: 0.667. loss: 0.748, lr: 0.000515490\n",
      "epoch: 9500, acc: 0.673. loss: 0.747, lr: 0.000512847\n",
      "epoch: 9600, acc: 0.673. loss: 0.746, lr: 0.000510230\n",
      "epoch: 9700, acc: 0.670. loss: 0.745, lr: 0.000507640\n",
      "epoch: 9800, acc: 0.670. loss: 0.744, lr: 0.000505076\n",
      "epoch: 9900, acc: 0.673. loss: 0.743, lr: 0.000502538\n",
      "epoch: 10000, acc: 0.677. loss: 0.742, lr: 0.000500025\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adam",
   "id": "a13e9bbd652694b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Short for **Adaptive Momentum**, is currently the most widely-used optimizer and is built atop RMSProp, with the momentum concept from SGD added back in. This means that, instead of applying current gradients, we're going to apply momentums like in the SGD optimizer with momentum, then apply a per-weight adaptive learning rate with the cache as done in RMSProp.\n",
    "\n",
    "The Adam optimizer additionally adds a bias correction mechanism. This is different from the layer's bias. The bias correction mechanism is applied to the cache and momentum, compensating for the initial zeroed values before they warm up with initial steps. To achieve this correction, both momentum and caches are divided by *1-beta^step*. As step raises, *beta^step* approaches *0* ( a fraction to the power of a rising value decreases), solving this whole expression to a fraction during the first steps and approaching *1* as training progreses. For example, *beta 1*, a fraction of momentum to apply, defaults to 0.9. this means that, during the first step, the correction value equals:\n",
    "\n",
    "<code>\n",
    "\n",
    "1 - 0.9**1 = 1 - 0.9 = 0.1\n",
    "\n",
    "</code>\n",
    "\n",
    "With training progression, a step count rises:\n",
    "\n",
    "<code>\n",
    "\n",
    "1 - lim_step->inf (0.9**1) = 1 - 0 = 1\n",
    "\n",
    "The same applies to the cache and the *beta 2* - in this case, the starting value is 0.001 and also approaches *1*. These values divide the momentums and the cache, respectively. Division by a fraction causes them to be multiple times bigger, significantly speeding up training in the initial stages before both tables warm up during multiple initial steps. We also previously mentioned that both of these bias-correcting coefficients go towards a value of *1* as training progresses and return parameter updates to their typical values for the later training steps. To get parameter updates, we divide the scaled momentum by the scaled square-rooted cache.\n",
    "\n",
    "The code for the Adam Optimizer is based on the RMSProp optimizer. It adds the cache seen from the SGD along with the *beta 1* hyperparameter. Next, it introduces the bias correction mechanism for both the momentum and the cache. We've also modified the way the parameter updates are calculated - using corrected momentums and corrected caches, instead of gradients and caches."
   ],
   "id": "29f927031722115b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T03:24:19.417554Z",
     "start_time": "2025-04-13T03:24:19.401989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialiize optimzier - set settings\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros,\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradient\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentums\n",
    "        # self.iterations is 0 at first pass and we need to start with 1\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with squared rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "71a2ce9db3263bd6",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following changes were made from copying the RAMSProp class code\n",
    "\n",
    "* renamed the *rho* hyperparameter and property to `beta_2` in `__init__`\n",
    "* added `beta_1` hyperparameter and property in `__init__`\n",
    "* added *momentum* array creation in `update_params()`\n",
    "* added *momentum* calculation\n",
    "* renamed `self.hro` to `self.beta_2` with cache calculation code in `update_params()`\n",
    "* added `*_corrected` variables as corrected momentums and weights\n",
    "* replaced `layer.dweights`, `layer.dbiases`, `layer.weight_cache`, and `layer.bias_cache` with corrected arrays of values in parameter updates with momentum arrays"
   ],
   "id": "661677e289c70b41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T03:29:09.048088Z",
     "start_time": "2025-04-13T03:28:45.493964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimizer.current_learning_rate}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ],
   "id": "8c8739801bba9a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.347, loss: 1.123, lr: 0.05\n",
      "epoch: 100, acc: 0.670, loss: 0.811, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.780, loss: 0.561, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.783, loss: 0.480, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.800, loss: 0.430, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.833, loss: 0.389, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.843, loss: 0.360, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.883, loss: 0.323, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.880, loss: 0.301, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.890, loss: 0.278, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.907, loss: 0.256, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.920, loss: 0.232, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.920, loss: 0.217, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.910, loss: 0.240, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.940, loss: 0.187, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.940, loss: 0.178, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.940, loss: 0.171, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.937, loss: 0.164, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.947, loss: 0.159, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.943, loss: 0.154, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.937, loss: 0.195, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.953, loss: 0.149, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.957, loss: 0.143, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.957, loss: 0.140, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.960, loss: 0.137, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.957, loss: 0.135, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.963, loss: 0.133, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.960, loss: 0.130, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.960, loss: 0.128, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.960, loss: 0.125, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.963, loss: 0.123, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.963, loss: 0.121, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.963, loss: 0.120, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.960, loss: 0.117, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.963, loss: 0.114, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.960, loss: 0.114, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.963, loss: 0.109, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.953, loss: 0.113, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.967, loss: 0.104, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.953, loss: 0.115, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.970, loss: 0.105, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.967, loss: 0.102, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.967, loss: 0.101, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.967, loss: 0.100, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.970, loss: 0.098, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.970, loss: 0.098, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.970, loss: 0.096, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.967, loss: 0.095, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.970, loss: 0.094, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.973, loss: 0.094, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.970, loss: 0.092, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.973, loss: 0.091, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.967, loss: 0.091, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.970, loss: 0.090, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.943, loss: 0.158, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.963, loss: 0.096, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.967, loss: 0.094, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.973, loss: 0.093, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.973, loss: 0.092, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.973, loss: 0.091, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.973, loss: 0.090, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.973, loss: 0.090, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.973, loss: 0.089, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.973, loss: 0.089, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.970, loss: 0.088, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.973, loss: 0.087, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.973, loss: 0.087, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.973, loss: 0.086, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.973, loss: 0.086, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.973, loss: 0.085, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.973, loss: 0.085, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.970, loss: 0.084, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.963, loss: 0.092, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.967, loss: 0.088, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.967, loss: 0.086, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.973, loss: 0.085, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.973, loss: 0.085, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.973, loss: 0.084, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.973, loss: 0.083, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.973, loss: 0.083, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.973, loss: 0.083, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.973, loss: 0.082, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.973, loss: 0.082, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.973, loss: 0.082, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.973, loss: 0.081, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.973, loss: 0.081, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.973, loss: 0.081, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.973, loss: 0.080, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.973, loss: 0.080, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.973, loss: 0.080, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.970, loss: 0.079, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.973, loss: 0.079, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.973, loss: 0.079, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.970, loss: 0.078, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.960, loss: 0.093, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.970, loss: 0.085, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.967, loss: 0.083, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.967, loss: 0.082, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.967, loss: 0.082, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.967, loss: 0.081, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.967, loss: 0.081, lr: 0.04975126853296942\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Full code up to this point",
   "id": "584a9827c08a69d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T03:23:12.920934Z",
     "start_time": "2025-04-13T03:23:12.872112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # initalize layer\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.rand(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remmeber input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU Activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variables,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradients where input values are negative or zeros\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalzied probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumberate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# SGD Optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # initialize optimizer - set settings\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self,\n",
    "                 learning=1.,\n",
    "                 decay=0,\n",
    "                 momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays\n",
    "            # create them fillted with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous updates\n",
    "            # multipled by retain factor and update with current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# AdaGrad Optimizer:\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=1.,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. / + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # update caches with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with square rotted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSProp optimzier\n",
    "class Optimizer_RMSProp:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them fillted with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization with squared rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum with current gradient\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        # Get corrected momentum\n",
    "        # self.iterations is 0 at first pass and we need to start with 1\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "       # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common Loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculate the dta and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean toward any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Numer of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Number of lables in every sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and crossentropy loss for faster back propagation\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ],
   "id": "53adfc3ffd2a530f",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T03:27:21.700505Z",
     "start_time": "2025-04-13T03:26:58.485898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Accuracy\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f} lr: {optimizer.current_learning_rate}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ],
   "id": "17faa7342b9de4a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.297, loss: 1.306 lr: 0.05\n",
      "epoch: 100, acc: 0.650, loss: 0.856 lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.697, loss: 0.627 lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.800, loss: 0.500 lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.860, loss: 0.385 lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.893, loss: 0.327 lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.913, loss: 0.273 lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.927, loss: 0.240 lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.933, loss: 0.210 lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.937, loss: 0.193 lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.937, loss: 0.179 lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.937, loss: 0.168 lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.940, loss: 0.158 lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.943, loss: 0.152 lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.947, loss: 0.148 lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.950, loss: 0.142 lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.950, loss: 0.141 lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.953, loss: 0.135 lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.950, loss: 0.133 lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.947, loss: 0.144 lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.950, loss: 0.130 lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.953, loss: 0.128 lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.950, loss: 0.125 lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.963, loss: 0.120 lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.967, loss: 0.116 lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.963, loss: 0.113 lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.967, loss: 0.111 lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.963, loss: 0.109 lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.963, loss: 0.107 lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.963, loss: 0.104 lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.963, loss: 0.102 lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.963, loss: 0.101 lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.967, loss: 0.098 lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.967, loss: 0.096 lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.960, loss: 0.096 lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.970, loss: 0.093 lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.967, loss: 0.091 lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.967, loss: 0.089 lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.967, loss: 0.089 lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.943, loss: 0.143 lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.970, loss: 0.088 lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.970, loss: 0.086 lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.970, loss: 0.085 lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.970, loss: 0.084 lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.970, loss: 0.083 lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.970, loss: 0.082 lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.970, loss: 0.081 lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.970, loss: 0.080 lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.970, loss: 0.079 lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.967, loss: 0.079 lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.973, loss: 0.077 lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.973, loss: 0.076 lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.973, loss: 0.075 lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.973, loss: 0.073 lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.973, loss: 0.072 lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.970, loss: 0.073 lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.973, loss: 0.070 lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.973, loss: 0.068 lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.953, loss: 0.109 lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.973, loss: 0.069 lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.973, loss: 0.067 lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.977, loss: 0.065 lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.977, loss: 0.064 lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.977, loss: 0.063 lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.977, loss: 0.062 lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.977, loss: 0.062 lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.977, loss: 0.061 lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.977, loss: 0.061 lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.977, loss: 0.060 lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.977, loss: 0.060 lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.977, loss: 0.059 lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.980, loss: 0.059 lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.980, loss: 0.058 lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.980, loss: 0.058 lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.980, loss: 0.057 lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.980, loss: 0.057 lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.977, loss: 0.056 lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.977, loss: 0.056 lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.983, loss: 0.055 lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.980, loss: 0.054 lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.977, loss: 0.054 lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.977, loss: 0.053 lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.983, loss: 0.053 lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.983, loss: 0.052 lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.983, loss: 0.052 lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.977, loss: 0.055 lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.983, loss: 0.054 lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.983, loss: 0.053 lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.983, loss: 0.053 lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.983, loss: 0.053 lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.983, loss: 0.052 lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.983, loss: 0.052 lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.983, loss: 0.052 lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.983, loss: 0.052 lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.983, loss: 0.051 lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.983, loss: 0.051 lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.983, loss: 0.051 lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.983, loss: 0.051 lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.983, loss: 0.051 lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.983, loss: 0.050 lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.983, loss: 0.050 lr: 0.04975126853296942\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7721083324354a18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
