{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Up until this notebook, we've used an output layer that is a probability distribution, where all of the values represent a confidence level of a given class being the correct class, and where these confidences sum to *1*.\n",
    "\n",
    "We're now going to cover an alternate output layer option, where each neuron separately represents two classes - 0 for one the classes, and a 1 for the other. A model with this type of output is called **binary logistic regression**. this single neuron could represent two classes like *cats* vs. *dogs*, but it could also represent *cat* vs. *not cat* or any combination of 2 classes, and we could have many of these.\n",
    "\n",
    "For example, a model may have two binary output neurons. One of these neurons could be distinguishing between *person/not person*, and the other neuron could be deciding between *indoors/outdoors*.\n",
    "\n",
    "Binary logistic regression is a regressor type of algorithm, which will differ as we'll use a **sigmoid** activation funtion for the output layer rather than softmax, and **binary cross-entropy** rather than categorical cross-entropy for calculating loss."
   ],
   "id": "8ea35a0ba392bdb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sigmoid Activation Function",
   "id": "343f7e2b1c8fc43f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The sigmoid function is used with regressors because it \"squishes\" a range of outputs from negative infinity to positive infinity to be between 0 and 1. The bounds represent thw two possible classes. The sigmoid equation is:\n",
    "\n",
    "$\n",
    "\\sigma_i,_j = \\frac{1}{1 + e^{-x}}\\\n",
    "$\n",
    "\n",
    "The denomiator of the Sigmoid function contains *e* raised to the power of *z_i,j*, where *z*, given indices, means a singular output value of the layer that this activation takes as input. The index *i* means the current sample, and the index *j* means the current output in this sample.\n"
   ],
   "id": "bd6fa37d4a6fd570"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:25:17.915202Z",
     "start_time": "2025-05-01T05:25:17.801202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x, y, label=r'$\\sigma(x) = \\frac{1}{1 + e^{-x}}$', color='blue', linewidth=2)\n",
    "\n",
    "plt.axhline(y=0, color='black', linewidth=0.5)  # x-axis line\n",
    "plt.axvline(x=0, color='black', linewidth=0.5)  # y-axis line\n",
    "\n",
    "plt.title('Sigmoid Function', fontsize=13)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel(r'$\\sigma(x)$', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ],
   "id": "13527553ab5a6f9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The sigmoid function approaches both maximum and minimum values exponentially fast. For example, for input of 2, output is ~0.8, 3 outputs ~0.95, -2 outputs -.12, -3 output ~0.05. This makes the sigmoid activation function a good candidate to apply to the final layer's output with a binary logistic regression model.\n",
    "\n",
    "For commonly-used functions, such as the sigmoid function, the derivative are almost always public knowledge. The sigmoid function's derivative solves to\n",
    "\n",
    "$\n",
    "\\sigma_i,_j(1 - \\sigma_i,_j)\n",
    "$"
   ],
   "id": "8004229f5faaa409"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sigmoid Function Derivative\n",
   "id": "c9890d09b89a258e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} \\Rightarrow \\frac{d}{dz} \\sigma(z) = \\frac{d}{dz} \\left( \\frac{1}{1 + e^{-z}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{d}{dz} \\left( 1 + e^{-z} \\right)^{-1} = -1 \\cdot \\left(1 + e^{-z} \\right)^{-2} \\cdot \\frac{d}{dz} (1 + e^{-z})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\left(1 + e^{-z} \\right)^{-2} \\cdot \\left( 0 + (-1)e^{-z} \\right) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\left( \\frac{1}{1 + e^{-z}} \\right) \\cdot \\left( \\frac{e^{-z}}{1 + e^{-z}} \\right) = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "$$\n"
   ],
   "id": "c5b7bcccba06206e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sigmoid Function Code",
   "id": "fba676db21347525"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:41:12.529121Z",
     "start_time": "2025-05-01T05:41:12.524937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Sigmoid:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ],
   "id": "6c6c87539fe5b1a1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Binary Cross-Entropy Loss\n",
    "\n",
    "To calculate binary cross-entropy loss, we will continue to use the negative log concep from categorical cross-entropy loss. Rather than only calculating this on the target class, we will sum the log-likelihoods of the correct and incorrect classes for each neuron separately. Because class values are either 0 or 1, we can simplify the incorrect class to be *1 - correct class* as this inverts the value. We can then calculate the negative log-likehood of the correct and incorrect classes, adding them together.\n",
    "\n",
    "$\n",
    "L_i,_j = (y_i,_j)(-log(y^_i_j)) + (1 - y_i,_j)(-log(1 - y^_i,_j))\n",
    "       = -y_i,_j.log(y^_i,_j) - (1 - y_i_j).log(1 - y^_i,_j)\n",
    "$\n",
    "\n",
    "Since a model can contain multiple binary outputs, and each of them, unlike in the cross-entropy loss, outputs its own prediction, loss calculated on a single output is going to be a vector of losses containing one value for each output. What we need is a sample loss and, to achieve that, we need to calculate a mean of all of these losses from a single sample.\n",
    "\n",
    "<code>\n",
    "\n",
    "sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "</code>\n",
    "\n",
    "The last parameter, `axis=-1`, informs NumPy to calculate the mean values along the last dimension. To make it easier to visualize, let's use a simple example. Assume that this is an output of the model containing 3 neurons in the output layer, and it's passed through the binary cross-entropy loss function:"
   ],
   "id": "fb3eab5bb035b856"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:54:27.973488Z",
     "start_time": "2025-05-01T05:54:27.969746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = np.array([[1, 2, 3],\n",
    "                    [2, 4, 6],\n",
    "                    [0, 5, 10],\n",
    "                    [11, 12, 13],\n",
    "                    [5, 10, 15]])\n"
   ],
   "id": "e374aae6c70c1cc2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We want to take each of the output vectors, [1, 2, 3] for example, and calculate a mean value from the numbers they hold, putting the result on the output vector. We then want to repeat this for the other vectors and return the resulting vector, which will ne a one-dimensional array:",
   "id": "7a6a3e4af50749"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:55:49.348382Z",
     "start_time": "2025-05-01T05:55:49.343878Z"
    }
   },
   "cell_type": "code",
   "source": "np.mean(outputs, axis=-1)",
   "id": "39ab436a42c3fc36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  4.,  5., 12., 10.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are also going to inherit from the **Loss** class, so the overall loss calculation will be handled by the `calculate` method that we already created for the categorical cross-entropy loss class.",
   "id": "27bdfeda03586b01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Binary Cross-Entropy Loss Derivative",
   "id": "bc2546696cffd551"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Binary Cross-Entropy Code\n",
    "\n",
    "In our code, it looks like:"
   ],
   "id": "ff9fb1065ceec15b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dvalues = outputs\n",
    "\n",
    "samples = len(dvalues)\n",
    "\n",
    "# Number of outputs in every sample\n",
    "# We'll use the first sample to count them\n",
    "outputs = len(dvalues[0])\n",
    "\n",
    "# Calculate gradient\n",
    "self.dinputs = -(y_true / clippe_dvalues - \\\n",
    "                 (1 - y_true) / (1 - clipped_dvalues)) / outputs\n"
   ],
   "id": "86200e4b1778158",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Similar to what we did in the categorical cross-entropy loss, we need to normalize gradient so it'll become invariant to the number of samples we calculate it for:\n",
   "id": "c1197ff83dc519d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "self.dinputs = self.dinputs / samples",
   "id": "6d885d1155dbf9b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we need to addres the numerial instability of the logarithmic function. The sigmoid activation can return a value in the range of *0* and *1* (inclusive), but the `log(0)` presents sligth issue due to how it's calculated and will return *negative infinity*. This along isn't necessarily a big deal, but any list with *-inf* in it will have a mean of *-inf*, which is the same for any list with positive infinity averaging to infinity.",
   "id": "90afbf0db053c62d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:58:31.223464Z",
     "start_time": "2025-05-02T04:58:31.212235Z"
    }
   },
   "cell_type": "code",
   "source": "np.log(0)",
   "id": "189da6e370588129",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linho\\AppData\\Local\\Temp\\ipykernel_13908\\1608527138.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-inf)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T04:58:45.623515Z",
     "start_time": "2025-05-02T04:58:45.614415Z"
    }
   },
   "cell_type": "code",
   "source": "print(np.mean([5, 2, 4, np.log(0)]))",
   "id": "9f055a9c4783b38e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linho\\AppData\\Local\\Temp\\ipykernel_13908\\2510640258.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  print(np.mean([5, 2, 4, np.log(0)]))\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is similar issue to the one we discussed earlier regarding categorical cross-entropy loss. To prevent this issue, we'll add clipping on the batch of values:",
   "id": "844185ae0ba8eaf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "# We use this for the forward pass as well\n",
    "sample_losses = -(y_true * np.log(y_pred_clipped) + \\\n",
    "                  (1 - y_true) * np.log(1 - y_pred_clipped))"
   ],
   "id": "d8360daa70180b86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we perform the division operation durint the derivative calculation, the gradient passed in may contain both values, *0* and *1*. Either of these values will cause a problem in either the `y_true / dvalues` or `(1 - y_true) / (1 - dvalues)` parts respectively (*0* in the first and *1-1=0* in the second case will also cause division by *0*, so need to clip this gradient as well:",
   "id": "e3075019eff4154d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T05:05:23.568771Z",
     "start_time": "2025-05-02T05:05:23.560345Z"
    }
   },
   "cell_type": "code",
   "source": "clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)",
   "id": "1c10a62ae99c49f7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9999999)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Lost_BinaryCrossentropy(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + \\\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        self.dinputs = -(y_true / clipped_dvalues - \\\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        self.dinputs = self.dinputs / samples"
   ],
   "id": "df785abc600b94d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementing Binary Logistic Regression and Binary Cross-Entropy Loss",
   "id": "2bf1fe06a2ff50cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With these new classes, our code changes will be in the executtion of actual code (instead of modifying the classes). The first is to make the `spiral_data` object output 2 classes, rather than 3:\n",
   "id": "97655c797c421398"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T05:12:35.426797Z",
     "start_time": "2025-05-02T05:12:35.293980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ],
   "id": "b0eef0c039ebf1b7",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T05:12:49.590222Z",
     "start_time": "2025-05-02T05:12:48.751130Z"
    }
   },
   "cell_type": "code",
   "source": "X, y = spiral_data(samples=100, classes=2)\n",
   "id": "ce570590a4cbc9a6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next. we'll reshape our labels, as they're not sparse anymore. They're binary, *0* or *1*:",
   "id": "a210808b255b306a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T05:14:36.007976Z",
     "start_time": "2025-05-02T05:14:35.999603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reshape labels to be a list of lists\n",
    "# inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y = y.reshape(-1, 1)\n",
    "print(y[:10])"
   ],
   "id": "1e85329c1e783aee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why have we done this? initially, with the softmax classifier, the values from the `spiral_data` could be used directly as the target labels, as they contain the correct class labels in the numerical form -- an index of the correct class, where each neuron in the output layer is a separate class, for example [0, 1, 1, 0, 1]. In this case, however, we're trying to represent some binary outputs, where each neurons represents 2 possible classes on its own. For the example we're currently working on, we have single output neuron so the output from our neural network should be a tensor (array), containing one value, of a target value of either *0* or *1*, for example [[0], [1], [1], [0], [1]].\n",
    "\n",
    "The `.reshape(-1, 1)` means to reshape the data into 2 dimensions, where the second dimension contains single element, and the first dimension contains how many elements the result will contain (-1) following other conditions. We are allowed to use -1 only once in a shape with NumPy, letting us have that dimension be variable. Thanks to this ability, we do not always need the same number of samples every time, and NumPY can handle the calculation for us. In the case above, they're all *-* because the `spiral_data` function makes the dataset one class at a time, starting with 0. We will also need to reshape the y-testing data in the same way.\n",
    "\n",
    "<code>\n",
    "\n",
    "X, y = spiral_data(100, 2)\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "bias_regularizer_l2=5e-4)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "</code>\n",
    "\n",
    "We're still going to use the **Rectified Linear Activation** for the hidden layer. The hidden layer activation functions don't necessarily need to change, even though we're effectively building a different type of classifier. Notice that because this is now a binary classifier, the `dense2` object has only 1 output. Its output represents exactly 2 classes (0 or 1) being mapped to one neuron. We can now select a loss function and optimizer. For the **Adam** optimizer settings, we are going to use the default learning rate and the dacaying of 5e-7.\n",
    "\n",
    "<code>\n",
    "\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(decay=5e-7)\n",
    "\n",
    "</code>\n",
    "\n",
    "Whie we require a different calculation for loss (since we use a differnt activation function for the output layer), we can still use the same optimizer as in the softmax classifier. Another small change is how we measure predictions. With probability distributions, we use `argmax` and determine which index is associated with the largest value, which becomes the classification result. With a binary classifier, we are determining if the output is closer to 0 or to 1. To do this, we simplify the output to:\n",
    "\n",
    "<code>\n",
    "\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "\n",
    "</code>\n",
    "\n",
    "This results in *True/False* evaluations to the statement that the output is above 0.5 for all values. *True* and *False*, when treated as numbers, are 1 and 0, respectively. For example, if we execute `int(True)`, the result will be 1. If we want to convert a list of True/False boolean values to numbers, we can't just wrap the list in `int()`. However, we can perform math operations directly on an array of boolean values and return the arithmetic aanswer.\n",
    "\n"
   ],
   "id": "98aec3d091b4f37a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T05:31:32.622169Z",
     "start_time": "2025-05-02T05:31:32.612522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = np.array([True, False, True])\n",
    "a"
   ],
   "id": "9ba7c114d7b09cb8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T05:31:42.851766Z",
     "start_time": "2025-05-02T05:31:42.842567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b = a*1\n",
    "b"
   ],
   "id": "9db68b7df93b25e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Thus, to evaluete predictive accuracy, we can:\n",
    "\n",
    "<code>\n",
    "\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "</code>\n",
    "\n",
    "The `* 1* multiplication turns an array of boolean True/False values into numerical 1/0 values, respectively. We will need to implement this accuracy calculation for validation data too."
   ],
   "id": "57a86973311c8d58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Full code up to this point",
   "id": "4eeff43b774654dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T05:34:39.631544Z",
     "start_time": "2025-05-02T05:34:39.624974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ],
   "id": "45fa3c8cb385c95b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:02:44.230829Z",
     "start_time": "2025-05-02T06:02:44.216397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer_Dense:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0,\n",
    "                 weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0,\n",
    "                 bias_regularizer_l2=0):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues,\n",
    "                              axis=0,\n",
    "                              keepdims=True)\n",
    "\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ],
   "id": "7fc75bc4678afa6e",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:02:54.538234Z",
     "start_time": "2025-05-02T06:02:54.530136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(np.random.rand(3, 3)\n",
    "      )"
   ],
   "id": "f2489c295db8c587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22308163 0.95274901 0.44712538]\n",
      " [0.84640867 0.69947928 0.29743695]\n",
      " [0.81379782 0.39650574 0.8811032 ]]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:02:58.725468Z",
     "start_time": "2025-05-02T06:02:58.717941Z"
    }
   },
   "cell_type": "code",
   "source": "print(np.zeros((1, 3)))",
   "id": "86810c9a5ee7d899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:05:15.945998Z",
     "start_time": "2025-05-02T06:05:15.937561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer_Dropout:\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = np.random.binomial(1,\n",
    "                                              self.rate,\n",
    "                                              size=inputs.shape) / \\\n",
    "                           self.rate\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ],
   "id": "643ecb10ad802647",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:06:40.232977Z",
     "start_time": "2025-05-02T06:06:40.224024Z"
    }
   },
   "cell_type": "code",
   "source": "print(np.random.binomial(1, 0.5, size=(3, 3)))",
   "id": "201f9a4bb450647b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:08:06.101922Z",
     "start_time": "2025-05-02T06:08:06.093886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ],
   "id": "54a09b87e7f64378",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T06:12:11.308154Z",
     "start_time": "2025-05-02T06:12:11.298440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs,\n",
    "                                            axis=1,\n",
    "                                            keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values,\n",
    "                                             axis=1,\n",
    "                                             keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                                np.dot(single_output, single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)"
   ],
   "id": "819c4e586498d9b7",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T20:27:04.615653Z",
     "start_time": "2025-05-03T20:27:04.607257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Sigmoid:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ],
   "id": "5d015e167b35c1e4",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T20:27:04.848259Z",
     "start_time": "2025-05-03T20:27:04.831079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=1.,\n",
    "                 decay=0.,\n",
    "                 momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                         (1. / (1. + self.decay + self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if self.momentum:\n",
    "\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                            layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                            layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "9261f83385f0ed7d",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T20:27:05.031077Z",
     "start_time": "2025-05-03T20:27:05.020646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_Adagrad:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=1.,\n",
    "                 decay=0,\n",
    "                 epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learining_rate = self.learning_rate * \\\n",
    "                                          (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ],
   "id": "26f872436fc0daed",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T20:27:05.388124Z",
     "start_time": "2025-05-03T20:27:05.376997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_RMSProp:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                         (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                             (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                           (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ],
   "id": "b5b922fd9e6f8ab5",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T06:59:57.880722Z",
     "start_time": "2025-05-04T06:59:57.866053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_Adam:\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.01,\n",
    "                 decay=0.,\n",
    "                 epsilon=1e-7,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                                         (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "                                     (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "                                   (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "                             (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "                           (1 - self.beta_2) * layer.dbiases ** 2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "                                 (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "                               (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        bias_momentums_corrected / \\\n",
    "                        (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "965a4faef03dcf73",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T20:27:05.791394Z",
     "start_time": "2025-05-03T20:27:05.781630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss:\n",
    "\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        regularization_loss = 0\n",
    "\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.weights))\n",
    "\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * layer.weights)\n",
    "\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.biases))\n",
    "\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                    np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n"
   ],
   "id": "a0ccfbcf265657b2",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T06:58:08.602064Z",
     "start_time": "2025-05-04T06:58:08.590811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n"
   ],
   "id": "75a5d11d37c1b099",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T06:58:23.390949Z",
     "start_time": "2025-05-04T06:58:23.381315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples\n"
   ],
   "id": "d189df9c7a729edd",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T06:58:30.810469Z",
     "start_time": "2025-05-04T06:58:30.801937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + \\\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        self.dinputs = -(y_true / clipped_dvalues - \\\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "\n",
    "        self.dinputs = self.dinputs / samples"
   ],
   "id": "4865b4123885b103",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T07:01:31.312070Z",
     "start_time": "2025-05-04T07:01:01.494156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = spiral_data(samples=1000, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains aone output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "dense1 = Layer_Dense(2, 64,\n",
    "                     weight_regularizer_l2=1e-4,\n",
    "                     bias_regularizer_l2=1e-4)\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(decay=5e-7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2)\n",
    "\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ],
   "id": "6e93a010580b657c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.500, data_loss: 1.863, reg_loss: 0.013, lr: 0.01\n",
      "epoch: 100, acc: 0.622, data_loss: 0.646, reg_loss: 0.012, lr: 0.009999505024501287\n",
      "epoch: 200, acc: 0.619, data_loss: 0.630, reg_loss: 0.011, lr: 0.009999005098992651\n",
      "epoch: 300, acc: 0.678, data_loss: 0.595, reg_loss: 0.011, lr: 0.009998505223469092\n",
      "epoch: 400, acc: 0.733, data_loss: 0.548, reg_loss: 0.013, lr: 0.009998005397923115\n",
      "epoch: 500, acc: 0.847, data_loss: 0.468, reg_loss: 0.017, lr: 0.009997505622347224\n",
      "epoch: 600, acc: 0.865, data_loss: 0.397, reg_loss: 0.023, lr: 0.00999700589673393\n",
      "epoch: 700, acc: 0.872, data_loss: 0.344, reg_loss: 0.030, lr: 0.009996506221075735\n",
      "epoch: 800, acc: 0.878, data_loss: 0.308, reg_loss: 0.035, lr: 0.00999600659536515\n",
      "epoch: 900, acc: 0.896, data_loss: 0.279, reg_loss: 0.040, lr: 0.009995507019594693\n",
      "epoch: 1000, acc: 0.899, data_loss: 0.261, reg_loss: 0.043, lr: 0.009995007493756867\n",
      "epoch: 1100, acc: 0.902, data_loss: 0.249, reg_loss: 0.044, lr: 0.009994508017844196\n",
      "epoch: 1200, acc: 0.906, data_loss: 0.239, reg_loss: 0.045, lr: 0.009994008591849186\n",
      "epoch: 1300, acc: 0.911, data_loss: 0.231, reg_loss: 0.046, lr: 0.00999350921576436\n",
      "epoch: 1400, acc: 0.917, data_loss: 0.223, reg_loss: 0.046, lr: 0.009993009889582237\n",
      "epoch: 1500, acc: 0.929, data_loss: 0.212, reg_loss: 0.046, lr: 0.009992510613295336\n",
      "epoch: 1600, acc: 0.947, data_loss: 0.195, reg_loss: 0.047, lr: 0.009992011386896175\n",
      "epoch: 1700, acc: 0.953, data_loss: 0.180, reg_loss: 0.048, lr: 0.009991512210377284\n",
      "epoch: 1800, acc: 0.954, data_loss: 0.169, reg_loss: 0.049, lr: 0.009991013083731183\n",
      "epoch: 1900, acc: 0.958, data_loss: 0.160, reg_loss: 0.050, lr: 0.009990514006950403\n",
      "epoch: 2000, acc: 0.959, data_loss: 0.153, reg_loss: 0.050, lr: 0.009990014980027462\n",
      "epoch: 2100, acc: 0.961, data_loss: 0.148, reg_loss: 0.050, lr: 0.009989516002954899\n",
      "epoch: 2200, acc: 0.961, data_loss: 0.143, reg_loss: 0.049, lr: 0.00998901707572524\n",
      "epoch: 2300, acc: 0.962, data_loss: 0.139, reg_loss: 0.048, lr: 0.009988518198331018\n",
      "epoch: 2400, acc: 0.962, data_loss: 0.135, reg_loss: 0.048, lr: 0.009988019370764769\n",
      "epoch: 2500, acc: 0.963, data_loss: 0.132, reg_loss: 0.047, lr: 0.009987520593019024\n",
      "epoch: 2600, acc: 0.963, data_loss: 0.130, reg_loss: 0.046, lr: 0.00998702186508632\n",
      "epoch: 2700, acc: 0.963, data_loss: 0.128, reg_loss: 0.045, lr: 0.0099865231869592\n",
      "epoch: 2800, acc: 0.963, data_loss: 0.126, reg_loss: 0.045, lr: 0.009986024558630197\n",
      "epoch: 2900, acc: 0.965, data_loss: 0.124, reg_loss: 0.044, lr: 0.009985525980091857\n",
      "epoch: 3000, acc: 0.965, data_loss: 0.122, reg_loss: 0.043, lr: 0.009985027451336722\n",
      "epoch: 3100, acc: 0.963, data_loss: 0.121, reg_loss: 0.042, lr: 0.009984528972357331\n",
      "epoch: 3200, acc: 0.964, data_loss: 0.120, reg_loss: 0.042, lr: 0.009984030543146238\n",
      "epoch: 3300, acc: 0.963, data_loss: 0.118, reg_loss: 0.041, lr: 0.009983532163695984\n",
      "epoch: 3400, acc: 0.963, data_loss: 0.117, reg_loss: 0.040, lr: 0.009983033833999119\n",
      "epoch: 3500, acc: 0.963, data_loss: 0.116, reg_loss: 0.040, lr: 0.009982535554048192\n",
      "epoch: 3600, acc: 0.964, data_loss: 0.115, reg_loss: 0.039, lr: 0.00998203732383576\n",
      "epoch: 3700, acc: 0.966, data_loss: 0.114, reg_loss: 0.039, lr: 0.009981539143354366\n",
      "epoch: 3800, acc: 0.964, data_loss: 0.113, reg_loss: 0.038, lr: 0.009981041012596573\n",
      "epoch: 3900, acc: 0.964, data_loss: 0.112, reg_loss: 0.037, lr: 0.009980542931554934\n",
      "epoch: 4000, acc: 0.966, data_loss: 0.111, reg_loss: 0.037, lr: 0.009980044900222007\n",
      "epoch: 4100, acc: 0.966, data_loss: 0.111, reg_loss: 0.036, lr: 0.009979546918590348\n",
      "epoch: 4200, acc: 0.967, data_loss: 0.109, reg_loss: 0.036, lr: 0.009979048986652524\n",
      "epoch: 4300, acc: 0.963, data_loss: 0.104, reg_loss: 0.036, lr: 0.00997855110440109\n",
      "epoch: 4400, acc: 0.961, data_loss: 0.103, reg_loss: 0.036, lr: 0.009978053271828614\n",
      "epoch: 4500, acc: 0.966, data_loss: 0.102, reg_loss: 0.035, lr: 0.009977555488927658\n",
      "epoch: 4600, acc: 0.971, data_loss: 0.100, reg_loss: 0.035, lr: 0.00997705775569079\n",
      "epoch: 4700, acc: 0.968, data_loss: 0.098, reg_loss: 0.035, lr: 0.009976560072110577\n",
      "epoch: 4800, acc: 0.969, data_loss: 0.097, reg_loss: 0.034, lr: 0.009976062438179587\n",
      "epoch: 4900, acc: 0.969, data_loss: 0.096, reg_loss: 0.034, lr: 0.009975564853890395\n",
      "epoch: 5000, acc: 0.968, data_loss: 0.095, reg_loss: 0.034, lr: 0.009975067319235571\n",
      "epoch: 5100, acc: 0.968, data_loss: 0.094, reg_loss: 0.033, lr: 0.009974569834207688\n",
      "epoch: 5200, acc: 0.970, data_loss: 0.093, reg_loss: 0.033, lr: 0.00997407239879932\n",
      "epoch: 5300, acc: 0.976, data_loss: 0.093, reg_loss: 0.033, lr: 0.009973575013003048\n",
      "epoch: 5400, acc: 0.972, data_loss: 0.092, reg_loss: 0.032, lr: 0.009973077676811447\n",
      "epoch: 5500, acc: 0.972, data_loss: 0.091, reg_loss: 0.032, lr: 0.009972580390217098\n",
      "epoch: 5600, acc: 0.973, data_loss: 0.091, reg_loss: 0.031, lr: 0.009972083153212581\n",
      "epoch: 5700, acc: 0.972, data_loss: 0.090, reg_loss: 0.031, lr: 0.00997158596579048\n",
      "epoch: 5800, acc: 0.973, data_loss: 0.090, reg_loss: 0.031, lr: 0.009971088827943378\n",
      "epoch: 5900, acc: 0.973, data_loss: 0.089, reg_loss: 0.030, lr: 0.009970591739663862\n",
      "epoch: 6000, acc: 0.973, data_loss: 0.089, reg_loss: 0.030, lr: 0.009970094700944516\n",
      "epoch: 6100, acc: 0.974, data_loss: 0.089, reg_loss: 0.030, lr: 0.009969597711777935\n",
      "epoch: 6200, acc: 0.974, data_loss: 0.088, reg_loss: 0.029, lr: 0.009969100772156701\n",
      "epoch: 6300, acc: 0.973, data_loss: 0.088, reg_loss: 0.029, lr: 0.00996860388207341\n",
      "epoch: 6400, acc: 0.974, data_loss: 0.088, reg_loss: 0.029, lr: 0.009968107041520654\n",
      "epoch: 6500, acc: 0.974, data_loss: 0.087, reg_loss: 0.029, lr: 0.00996761025049103\n",
      "epoch: 6600, acc: 0.974, data_loss: 0.086, reg_loss: 0.028, lr: 0.009967113508977131\n",
      "epoch: 6700, acc: 0.973, data_loss: 0.086, reg_loss: 0.028, lr: 0.009966616816971556\n",
      "epoch: 6800, acc: 0.973, data_loss: 0.086, reg_loss: 0.028, lr: 0.0099661201744669\n",
      "epoch: 6900, acc: 0.974, data_loss: 0.085, reg_loss: 0.028, lr: 0.009965623581455767\n",
      "epoch: 7000, acc: 0.974, data_loss: 0.085, reg_loss: 0.027, lr: 0.009965127037930762\n",
      "epoch: 7100, acc: 0.975, data_loss: 0.084, reg_loss: 0.027, lr: 0.009964630543884481\n",
      "epoch: 7200, acc: 0.974, data_loss: 0.084, reg_loss: 0.027, lr: 0.009964134099309536\n",
      "epoch: 7300, acc: 0.975, data_loss: 0.083, reg_loss: 0.027, lr: 0.009963637704198527\n",
      "epoch: 7400, acc: 0.974, data_loss: 0.083, reg_loss: 0.027, lr: 0.009963141358544066\n",
      "epoch: 7500, acc: 0.975, data_loss: 0.082, reg_loss: 0.026, lr: 0.00996264506233876\n",
      "epoch: 7600, acc: 0.975, data_loss: 0.082, reg_loss: 0.026, lr: 0.009962148815575222\n",
      "epoch: 7700, acc: 0.975, data_loss: 0.082, reg_loss: 0.026, lr: 0.009961652618246062\n",
      "epoch: 7800, acc: 0.974, data_loss: 0.081, reg_loss: 0.026, lr: 0.009961156470343895\n",
      "epoch: 7900, acc: 0.975, data_loss: 0.081, reg_loss: 0.026, lr: 0.009960660371861333\n",
      "epoch: 8000, acc: 0.974, data_loss: 0.081, reg_loss: 0.025, lr: 0.009960164322791\n",
      "epoch: 8100, acc: 0.975, data_loss: 0.080, reg_loss: 0.025, lr: 0.009959668323125503\n",
      "epoch: 8200, acc: 0.975, data_loss: 0.080, reg_loss: 0.025, lr: 0.009959172372857471\n",
      "epoch: 8300, acc: 0.975, data_loss: 0.080, reg_loss: 0.025, lr: 0.009958676471979521\n",
      "epoch: 8400, acc: 0.974, data_loss: 0.080, reg_loss: 0.025, lr: 0.009958180620484277\n",
      "epoch: 8500, acc: 0.974, data_loss: 0.079, reg_loss: 0.025, lr: 0.00995768481836436\n",
      "epoch: 8600, acc: 0.974, data_loss: 0.079, reg_loss: 0.024, lr: 0.009957189065612401\n",
      "epoch: 8700, acc: 0.974, data_loss: 0.079, reg_loss: 0.024, lr: 0.00995669336222102\n",
      "epoch: 8800, acc: 0.975, data_loss: 0.078, reg_loss: 0.024, lr: 0.00995619770818285\n",
      "epoch: 8900, acc: 0.975, data_loss: 0.078, reg_loss: 0.024, lr: 0.009955702103490519\n",
      "epoch: 9000, acc: 0.975, data_loss: 0.078, reg_loss: 0.024, lr: 0.009955206548136659\n",
      "epoch: 9100, acc: 0.976, data_loss: 0.078, reg_loss: 0.024, lr: 0.009954711042113902\n",
      "epoch: 9200, acc: 0.976, data_loss: 0.077, reg_loss: 0.024, lr: 0.009954215585414884\n",
      "epoch: 9300, acc: 0.975, data_loss: 0.077, reg_loss: 0.023, lr: 0.00995372017803224\n",
      "epoch: 9400, acc: 0.976, data_loss: 0.077, reg_loss: 0.023, lr: 0.009953224819958605\n",
      "epoch: 9500, acc: 0.975, data_loss: 0.077, reg_loss: 0.023, lr: 0.00995272951118662\n",
      "epoch: 9600, acc: 0.978, data_loss: 0.078, reg_loss: 0.023, lr: 0.009952234251708922\n",
      "epoch: 9700, acc: 0.974, data_loss: 0.076, reg_loss: 0.023, lr: 0.009951739041518158\n",
      "epoch: 9800, acc: 0.976, data_loss: 0.076, reg_loss: 0.023, lr: 0.009951243880606966\n",
      "epoch: 9900, acc: 0.975, data_loss: 0.077, reg_loss: 0.023, lr: 0.009950748768967994\n",
      "epoch: 10000, acc: 0.976, data_loss: 0.076, reg_loss: 0.022, lr: 0.009950253706593883\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T07:01:45.235810Z",
     "start_time": "2025-05-04T07:01:45.221244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test, y_test = spiral_data(samples=1000, classes=2)\n",
    "\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}. loss: {loss:.3f}')"
   ],
   "id": "478fc3c8fb84c53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.971. loss: 0.089\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T20:27:14.216576Z",
     "start_time": "2025-05-03T20:27:14.212205Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3b3376cf8362b02f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T20:27:14.265953Z",
     "start_time": "2025-05-03T20:27:14.261753Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a3b9a1184c0dd4c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "974de3ad3cfb8c55"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
